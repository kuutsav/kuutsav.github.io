<!DOCTYPE html>
<html lang="en-us">
    
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="theme-color" content="dark">
    <title>Revisiting Word2Vec skip-gram model | Utsav&#39;Log</title>

    
    
    
    <meta property="og:site_name" content="Utsav&#39;Log" />
    <meta property="og:title" content="Revisiting Word2Vec skip-gram model | Utsav&#39;Log"/>
    <meta itemprop="name" content="Revisiting Word2Vec skip-gram model | Utsav&#39;Log" />
    <meta name="twitter:title" content="Revisiting Word2Vec skip-gram model | Utsav&#39;Log" />
    <meta name="application-name" content="Revisiting Word2Vec skip-gram model | Utsav&#39;Log" /><meta name="twitter:card" content="summary"/>

    <meta name="description" content="ML and Backend stuff" />
    <meta name="twitter:description" content="ML and Backend stuff"/>
    <meta itemprop="description" content="ML and Backend stuff"/>
    <meta property="og:description" content="ML and Backend stuff" />

    


    <link rel="shortcut icon" type="image/x-icon" href="/static/favicon.ico" />
    
    <link rel="stylesheet" href="/sass/main.min.3f6178155abae4dffe563223f47300954fb71d4b007be75733f26fa1f8524296.css">
    
</head>
    
    <script>
        (function() {
            const colorSchemeKey = 'ThemeColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'ThemeColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.userColorScheme = 'dark';
        } else {
            document.documentElement.dataset.userColorScheme = 'light';
        }
    })();
</script>


    <body class="dark">
        <nav class="navbar">
    <div class="container">
        <div class="flex">
            <div>
                <a class="brand" href="/">
                    
                    
                        <img src="/favicon.ico" />
                    
                    Utsav&#39;Log
                    </a>
            </div>
            <div class="flex">
                
                
                    <button id="dark-mode-button">
                    <svg class="light" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" width="1em" height="1em" style="-ms-transform: rotate(360deg); -webkit-transform: rotate(360deg); transform: rotate(360deg);" preserveAspectRatio="xMidYMid meet" viewBox="0 0 36 36"><path fill="#FFD983" d="M30.312.776C32 19 20 32 .776 30.312c8.199 7.717 21.091 7.588 29.107-.429C37.9 21.867 38.03 8.975 30.312.776z"/><path d="M30.705 15.915a1.163 1.163 0 1 0 1.643 1.641a1.163 1.163 0 0 0-1.643-1.641zm-16.022 14.38a1.74 1.74 0 0 0 0 2.465a1.742 1.742 0 1 0 0-2.465zm13.968-2.147a2.904 2.904 0 0 1-4.108 0a2.902 2.902 0 0 1 0-4.107a2.902 2.902 0 0 1 4.108 0a2.902 2.902 0 0 1 0 4.107z" fill="#FFCC4D"/><rect x="0" y="0" width="36" height="36" fill="rgba(0, 0, 0, 0)" /></svg>
                    <svg class="dark" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" width="1em" height="1em" style="-ms-transform: rotate(360deg); -webkit-transform: rotate(360deg); transform: rotate(360deg);" preserveAspectRatio="xMidYMid meet" viewBox="0 0 36 36"><path fill="#FFD983" d="M16 2s0-2 2-2s2 2 2 2v2s0 2-2 2s-2-2-2-2V2zm18 14s2 0 2 2s-2 2-2 2h-2s-2 0-2-2s2-2 2-2h2zM4 16s2 0 2 2s-2 2-2 2H2s-2 0-2-2s2-2 2-2h2zm5.121-8.707s1.414 1.414 0 2.828s-2.828 0-2.828 0L4.878 8.708s-1.414-1.414 0-2.829c1.415-1.414 2.829 0 2.829 0l1.414 1.414zm21 21s1.414 1.414 0 2.828s-2.828 0-2.828 0l-1.414-1.414s-1.414-1.414 0-2.828s2.828 0 2.828 0l1.414 1.414zm-.413-18.172s-1.414 1.414-2.828 0s0-2.828 0-2.828l1.414-1.414s1.414-1.414 2.828 0s0 2.828 0 2.828l-1.414 1.414zm-21 21s-1.414 1.414-2.828 0s0-2.828 0-2.828l1.414-1.414s1.414-1.414 2.828 0s0 2.828 0 2.828l-1.414 1.414zM16 32s0-2 2-2s2 2 2 2v2s0 2-2 2s-2-2-2-2v-2z"/><circle fill="#FFD983" cx="18" cy="18" r="10"/><rect x="0" y="0" width="36" height="36" fill="rgba(0, 0, 0, 0)" /></svg>
                    </button>
                
            </div>
            </div>
    </div>
</nav>

        <main>
            
<div class="container">
    <article>
        <header class="article-header">
            <div class="thumb">
                <div>
                    <h1>Revisiting Word2Vec skip-gram model</h1>
                    <div class="post-meta">
                        <div>
                            
                            
                              
                            
                             <time>March 12, 2021</time>
                             Â· 9 minutes
                        </div>
                        <div class="tags">
                            
                            <a href="/tags/python/">#python</a>
                            
                            <a href="/tags/word2vec/">#word2vec</a>
                            
                            <a href="/tags/skipgram/">#skipgram</a>
                            
                        </div>
                    </div>
                </div>
            </div>
        </header>
    </article>

    <div class="article-post">
    <p>In this post, we will train a <code>Word2Vec skip-gram</code> model from scratch on some text and inspect the trained embeddings at the end.</p>
<p>The first step for using any kind of NLP pipeline is to vectorize the text. Traditionally, we used to do this using one-hot representation of vectors. This had various downsides like:</p>
<ul>
<li>Vectors tend to be very long as their size depends on the vocabulary size of the corpus(which grows with the corpus size).</li>
<li>They don&rsquo;t have any <code>understanding</code> of the text.</li>
<li>They are sparse and can&rsquo;t be used for any comparison as any two one-hot encoded vector from a set will be orthogonal.</li>
</ul>
<p>Word2vec is able to tackle all these challenges. The architecture we define later will enable us to learn a <code>distributed representation</code> for each word/token in our corpus. The key idea being, we can represent a word by adding contextual information to it. Context refers to the words/tokens neighbouring the word/token of interest to us.</p>
<p>Of course this technique is not perfect and has it&rsquo;s own downsides. The key downside being we loose the word/token ordering while training the word2vec model, hence we are essentially dealing with a <code>bag of words</code> model. This makes the resulting embeddings not suitable for sentence level representations. We will cover other techniques later (RNNs, Transformers) that produce embeddings more suitable for sentences. For now our sole focus will be learning good word/token level embeddings.</p>
<h3 id="first-we-will-import-the-necessary-libraries">
    <a href="#first-we-will-import-the-necessary-libraries" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    First we will import the necessary libraries
</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
</code></pre></td></tr></table>
</div>
</div><p>The whole process of training the embeddings can be broken down into the following key steps:</p>
<ol>
<li>We define a class <code>TextProcessor</code> that has methods to pre-process the text.</li>
<li><code>fetch_text()</code> fetches the texts directly from urls containing our text data.</li>
<li><code>process_text()</code> applies basic pre-procesing steps to the text and creates tokens using white space tokenization.</li>
<li><code>prepare_vocab()</code> creates token to index mapping and vice versa.</li>
<li>Create torch <code>Dataset</code> and <code>Dataloader</code> where each index in the dataset will be a tuple, <code>(center_word_ix, context_word_ix)</code>; i.e. pairs of indices of center word and context based on a pre defined window size.
<ol>
<li>The window side determines the number of context words around a center word.</li>
<li>For example, window size of 5 will create the follwing center and context pairs for the sentence &ldquo;a quick brown fox jumps&rdquo; with &ldquo;brown&rdquo; being the center word - <code>[(&quot;brown&quot;, &quot;a&quot;), (&quot;brown&quot;, &quot;quick&quot;), (&quot;brown&quot;, &quot;fox&quot;), (&quot;brown&quot;, &quot;jumps&quot;)]</code></li>
</ol>
</li>
<li>Create our <code>SkipGramModel</code> model by defining a custom model on top of the <code>torch.nn.Module</code>.</li>
<li>Define our loss function, optimizer and scheduler.</li>
<li>Train the model for a certain number of epochs.</li>
</ol>
<p>Finally, we will create a similarity matrix(cosine similarity) using the trained embeddings and explore similar embeddings for some word query.</p>
<h3 id="fetching-and-preparing-the-data">
    <a href="#fetching-and-preparing-the-data" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    Fetching and preparing the data
</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># We will be using the first four Harry Potter books.</span>
<span class="n">BASE_URL</span> <span class="o">=</span> <span class="s2">&#34;https://raw.githubusercontent.com/formcept/&#34;</span>
<span class="n">BASE_URL</span> <span class="o">+=</span> <span class="s2">&#34;whiteboard/master/nbviewer/notebooks/data/harrypotter&#34;</span>

<span class="n">BOOK_URLS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">BASE_URL</span><span class="si">}</span><span class="s2">/Book%201%20-%20The%20Philosopher&#39;s%20Stone.txt&#34;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">BASE_URL</span><span class="si">}</span><span class="s2">/Book%202%20-%20The%20Chamber%20of%20Secrets.txt&#34;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">BASE_URL</span><span class="si">}</span><span class="s2">/Book%203%20-%20The%20Prisoner%20of%20Azkaban.txt&#34;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">BASE_URL</span><span class="si">}</span><span class="s2">/Book%204%20-%20The%20Goblet%20of%20Fire.txt&#34;</span>
<span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">TextProcessor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">urls</span> <span class="o">=</span> <span class="n">urls</span>

    <span class="k">def</span> <span class="nf">fetch_text</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">url</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">urls</span><span class="p">):</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">text</span> <span class="o">+=</span> <span class="s2">&#34;</span><span class="se">\n\n\n</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="n">r</span><span class="o">.</span><span class="n">text</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Fetched </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">urls</span><span class="p">)</span><span class="si">}</span><span class="s2"> urls ...&#34;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_text</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Processing text ...&#34;</span><span class="p">)</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">{2,}&#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span><span class="si">}</span><span class="s2"> sentences&#34;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clean_sentences</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">txt</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">,</span> <span class="s2">&#34;&#34;</span><span class="p">)</span>
            <span class="n">txt</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s2">&#34;\w+&#34;</span><span class="p">,</span> <span class="n">txt</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">txt</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">clean_sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>

        <span class="c1"># Tokens and token counter will be used later to create a vocabulary for</span>
        <span class="c1"># us which will be used to map words/tokens to indices and vice versa, to</span>
        <span class="c1"># create training data and recreate the texts from predictions respectively.</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span><span class="si">}</span><span class="s2"> filtered sentences&#34;</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clean_sentences</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34; &#34;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens&#34;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_counter</span><span class="p">)</span><span class="si">}</span><span class="s2"> unique tokens&#34;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">prepare_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># min_count is the minimum number of times a token should appear in the</span>
        <span class="c1"># text to be considered in the vocabulary else they are assigned to a</span>
        <span class="c1"># default index which is equal to `len(vocab)`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2ix</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">token_counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_counter</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="n">min_count</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">w2ix</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ix2w</span> <span class="o">=</span> <span class="p">{</span><span class="n">ix</span><span class="p">:</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">ix</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2ix</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="c1"># Assign default index to rest of the tokens</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w2ix</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_counter</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2ix</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">w2ix</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ix2w</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;&lt;unk&gt;&#34;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Vocab size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text_processor</span> <span class="o">=</span> <span class="n">TextProcessor</span><span class="p">(</span><span class="n">BOOK_URLS</span><span class="p">)</span>
<span class="n">text_processor</span><span class="o">.</span><span class="n">fetch_text</span><span class="p">()</span>
<span class="n">text_processor</span><span class="o">.</span><span class="n">process_text</span><span class="p">()</span>
<span class="n">text_processor</span><span class="o">.</span><span class="n">prepare_vocab</span><span class="p">(</span><span class="n">min_count</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>Fetched 1/4 urls ...
Fetched 2/4 urls ...
Fetched 3/4 urls ...
Fetched 4/4 urls ...

Processing text ...
20033 sentences
20033 filtered sentences
501854 tokens
15078 unique tokens

Vocab size: 2103
</code></pre>
<h3 id="preparing-the-torch-dataset-and-dataloader">
    <a href="#preparing-the-torch-dataset-and-dataloader" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    Preparing the torch Dataset and Dataloader
</h3>
<p>The Dataloader will enable us to send data in batches during the forward pass of the model training.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">HarryPotterDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_processor</span><span class="p">:</span> <span class="n">TextProcessor</span><span class="p">,</span>  <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_processor</span> <span class="o">=</span> <span class="n">text_processor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_data</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">_init_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">txt</span> <span class="ow">in</span> <span class="n">text_processor</span><span class="o">.</span><span class="n">clean_sentences</span><span class="p">:</span>
            <span class="n">splits</span> <span class="o">=</span> <span class="n">txt</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34; &#34;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">window_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">center_word</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">window_words</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">window_size</span><span class="p">:</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">window_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">window_words</span><span class="p">:</span>
                    <span class="c1"># Each data point under self.data will be a tuple with index 0</span>
                    <span class="c1"># containing the index(w2ix) for center_word and</span>
                    <span class="c1"># index 1 containing the index(w2ix) for context_word</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">text_processor</span><span class="o">.</span><span class="n">w2ix</span><span class="p">[</span><span class="n">center_word</span><span class="p">],</span> <span class="n">text_processor</span><span class="o">.</span><span class="n">w2ix</span><span class="p">[</span><span class="n">context</span><span class="p">])</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ix</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
  
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dataset</span> <span class="o">=</span> <span class="n">HarryPotterDataset</span><span class="p">(</span><span class="n">text_processor</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>2489461
</code></pre>
<h3 id="creating-the-word2vec-model-using-torchnnmodule">
    <a href="#creating-the-word2vec-model-using-torchnnmodule" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    Creating the Word2Vec model using torch.nn.Module
</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SkipGramModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><p>The model architecure is <code>Embedding -&gt; Linear -&gt; Softmax</code>.</p>
<p><code>Embedding</code> is nothing but &ldquo;A simple lookup table that stores embeddings of a fixed
dictionary and size&rdquo; - pytorch documentation.</p>
<p>Based on our model architecture, this layer will contain the trained token/word
embeddings of interest to us and we will discard the weight matrix from
the <code>Linear</code> layer.</p>
<p>In each forward pass we:</p>
<ol>
<li>Pass indices of center word as input <code>x</code>.</li>
<li>These indices are looked up in the <code>self.embedding</code> table.</li>
<li>We transform the vector from last step using the Linear layer to get another vector with dimension <code>vocab_size</code>.</li>
<li>Finally we apply softmax and calcualte loss.</li>
</ol>
<p>A trained model should output high values for the <code>context indices</code> for a given
center word.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SkipGramModel</span><span class="p">(</span><span class="n">text_processor</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="defining-our-loss-function-optimizer-and-scheduler">
    <a href="#defining-our-loss-function-optimizer-and-scheduler" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    Defining our loss function, optimizer and scheduler
</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">N_EPOCHS</span> <span class="o">=</span>       <span class="mi">10</span>
<span class="n">STEP_TO_LOG</span> <span class="o">=</span> <span class="mi">10000</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="training-the-model">
    <a href="#training-the-model" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    Training the model
</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">epoch_losses</span><span class="p">,</span> <span class="n">batch_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_EPOCHS</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">STEP_TO_LOG</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">; steps=</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span><span class="si">}</span><span class="s2">; loss=</span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">&lt;.5f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">batch_losses</span> <span class="o">+=</span> <span class="n">losses</span>
    <span class="n">epoch_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> ; lr=</span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">_last_lr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">; loss=</span><span class="si">{</span><span class="n">epoch_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;.5f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>epoch 1; steps=    0/38898; loss=7.80020
epoch 1; steps=10000/38898; loss=5.55375
epoch 1; steps=20000/38898; loss=5.58139
epoch 1; steps=30000/38898; loss=5.27452
epoch 1 ; lr=0.005; loss=5.60440

epoch 2; steps=    0/38898; loss=4.85012
epoch 2; steps=10000/38898; loss=5.81531
epoch 2; steps=20000/38898; loss=5.07261
epoch 2; steps=30000/38898; loss=5.62864
epoch 2 ; lr=0.005; loss=5.52841

epoch 3; steps=    0/38898; loss=5.27015
epoch 3; steps=10000/38898; loss=5.55336
epoch 3; steps=20000/38898; loss=5.67527
epoch 3; steps=30000/38898; loss=5.60372
epoch 3 ; lr=0.005; loss=5.52233

epoch 4; steps=    0/38898; loss=5.50681
epoch 4; steps=10000/38898; loss=5.48357
epoch 4; steps=20000/38898; loss=5.18897
epoch 4; steps=30000/38898; loss=5.74382
epoch 4 ; lr=0.005; loss=5.52111

epoch 5; steps=    0/38898; loss=5.67365
epoch 5; steps=10000/38898; loss=5.66463
epoch 5; steps=20000/38898; loss=5.82155
epoch 5; steps=30000/38898; loss=5.77744
epoch 5 ; lr=0.0025; loss=5.52081

epoch 6; steps=    0/38898; loss=5.54513
epoch 6; steps=10000/38898; loss=5.40478
epoch 6; steps=20000/38898; loss=5.71839
epoch 6; steps=30000/38898; loss=5.23344
epoch 6 ; lr=0.0025; loss=5.47719

epoch 7; steps=    0/38898; loss=4.93150
epoch 7; steps=10000/38898; loss=5.09374
epoch 7; steps=20000/38898; loss=5.47905
epoch 7; steps=30000/38898; loss=5.92455
epoch 7 ; lr=0.0025; loss=5.47128

epoch 8; steps=    0/38898; loss=5.22414
epoch 8; steps=10000/38898; loss=5.67435
epoch 8; steps=20000/38898; loss=5.55853
epoch 8; steps=30000/38898; loss=5.28791
epoch 8 ; lr=0.0025; loss=5.47099

epoch 9; steps=    0/38898; loss=5.33284
epoch 9; steps=10000/38898; loss=5.80998
epoch 9; steps=20000/38898; loss=6.04806
epoch 9; steps=30000/38898; loss=5.58979
epoch 9 ; lr=0.0025; loss=5.47106

epoch 10; steps=    0/38898; loss=5.09184
epoch 10; steps=10000/38898; loss=5.18391
epoch 10; steps=20000/38898; loss=5.66316
epoch 10; steps=30000/38898; loss=5.44824
epoch 10 ; lr=0.00125; loss=5.47111
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">weights</span><span class="o">.</span><span class="n">shape</span>
</code></pre></td></tr></table>
</div>
</div><pre><code>(2103, 30)
</code></pre>
<h3 id="creating-the-similarity-matrix">
    <a href="#creating-the-similarity-matrix" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    Creating the similarity matrix
</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_similar_words</span><span class="p">(</span><span class="n">input_word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">input_word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">text_processor</span><span class="o">.</span><span class="n">w2ix</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;word not in vocab&#34;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_word_ix</span> <span class="o">=</span> <span class="n">text_processor</span><span class="o">.</span><span class="n">w2ix</span><span class="p">[</span><span class="n">input_word</span><span class="p">]</span>
        <span class="n">similarity_vector</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">[</span><span class="n">input_word_ix</span><span class="p">]</span>
        <span class="n">most_similar_ixs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">similarity_vector</span><span class="p">)[</span><span class="o">-</span><span class="n">n</span><span class="p">:][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">most_similar_words</span> <span class="o">=</span> <span class="p">[(</span><span class="n">text_processor</span><span class="o">.</span><span class="n">ix2w</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">similarity_vector</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
                              <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="n">most_similar_ixs</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">most_similar_words</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34; &gt; </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">, score=</span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="exploring-similar-embeddings-for-some-queries">
    <a href="#exploring-similar-embeddings-for-some-queries" class="anchor">
        <svg class="icon" aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg>
    </a>
    Exploring similar embeddings for some queries
</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">get_similar_words</span><span class="p">(</span><span class="s2">&#34;snape&#34;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code> &gt; snape, score= 1.00
 &gt; karkaroff, score= 0.81
 &gt; dumbledore, score= 0.79
 &gt; lupin, score= 0.77
 &gt; mcgonagall, score= 0.76
 &gt; moody, score= 0.74
 &gt; professor, score= 0.72
 &gt; flitwick, score= 0.71
 &gt; trelawney, score= 0.69
 &gt; quirrell, score= 0.69
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">get_similar_words</span><span class="p">(</span><span class="s2">&#34;vernon&#34;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code> &gt; vernon, score= 1.00
 &gt; aunt, score= 0.81
 &gt; uncle, score= 0.79
 &gt; dudley, score= 0.78
 &gt; petunia, score= 0.70
 &gt; marge, score= 0.62
 &gt; furious, score= 0.59
 &gt; telephone, score= 0.53
 &gt; sister, score= 0.53
 &gt; company, score= 0.52
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">get_similar_words</span><span class="p">(</span><span class="s2">&#34;muggle&#34;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code> &gt; muggle, score= 1.00
 &gt; wizarding, score= 0.81
 &gt; old, score= 0.65
 &gt; yer, score= 0.63
 &gt; international, score= 0.61
 &gt; important, score= 0.61
 &gt; our, score= 0.61
 &gt; is, score= 0.61
 &gt; young, score= 0.60
 &gt; yourself, score= 0.60
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">get_similar_words</span><span class="p">(</span><span class="s2">&#34;harry&#34;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code> &gt; harry, score= 1.00
 &gt; lupin, score= 0.64
 &gt; ron, score= 0.63
 &gt; hermione, score= 0.61
 &gt; she, score= 0.57
 &gt; he, score= 0.57
 &gt; colin, score= 0.54
 &gt; dumbledore, score= 0.52
 &gt; furiously, score= 0.51
 &gt; moody, score= 0.49
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">get_similar_words</span><span class="p">(</span><span class="s2">&#34;slytherin&#34;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code> &gt; slytherin, score= 1.00
 &gt; gryffindor, score= 0.79
 &gt; hufflepuff, score= 0.75
 &gt; team, score= 0.71
 &gt; heir, score= 0.71
 &gt; ravenclaw, score= 0.66
 &gt; goal, score= 0.64
 &gt; seeker, score= 0.64
 &gt; wood, score= 0.60
 &gt; irish, score= 0.60
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">get_similar_words</span><span class="p">(</span><span class="s2">&#34;malfoy&#34;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><pre><code> &gt; malfoy, score= 1.00
 &gt; goyle, score= 0.77
 &gt; crabbe, score= 0.64
 &gt; draco, score= 0.61
 &gt; diggory, score= 0.59
 &gt; lucius, score= 0.57
 &gt; weasley, score= 0.51
 &gt; laughing, score= 0.50
 &gt; pettigrew, score= 0.50
 &gt; percy, score= 0.50
</code></pre>
<p>There are various Intrisic and Extrinsic evaluation criterias for these embeddings.
For <code>Evaluation</code> and <code>Interpretation</code> of the embeddings take a look <a href="https://lena-voita.github.io/nlp_course/word_embeddings.html#evaluation">here</a>.</p>

    </div>
</div>

<div class="container">
    
    <nav class="flex container suggested">
        
        
        
        <a rel="next" href="/posts/mlops_template_1_setup/" title="Next post (newer)">
            <span>Next</span>
            MLOps Template, Part 1 - Setup
            </a> 
        
    </nav>
    
</div>
 
<div class="container">
    
    <script src="https://giscus.app/client.js" 
        data-repo="kuutsav/kuutsav.github.io"
        data-repo-id="R_kgDOHvsC_Q"
        
        data-category="Comments"
        data-category-id="DIC_kwDOHvsC_c4CQiYb"
        
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-theme="light"
        crossorigin="anonymous"
        async
        >
</script>

<script>
    function setGiscusTeheme(theme) {
        let giscus = document.querySelector('.giscus iframe');
        if (giscus) {
            giscus.contentWindow.postMessage(
                {
                    giscus: {
                        setConfig: {
                            theme: theme
                        }
                    }
                },
                'https://giscus.app'
            )
        }
    }

    addEventListener('message', event => {
        if (event.origin !== 'https://giscus.app') return;
        setGiscusTeheme(document.documentElement.dataset.userColorScheme)
    });

    window.addEventListener('onColorSchemeChange', (e) => {
        setGiscusTeheme(e.detail)
    })
</script>

</div>

</main>


        </main>
        <footer class="footer flex">
    <section class="container">
        <nav class="footer-links">
            
            <a href="/index.xml">RSS</a>
            
            <a href="https://gohugo.io/">Â© 2022 Powered by Hugo</a>
            
        </nav>

        
    </section>
    <script defer src="/ts/features.aeccc1b543daef772e7584f28d3bc3e21c62dfd259256c435908dbe148b24c6c.js" 
    data-enable-footnotes="true"
    ></script>
</footer>

    </body>
</html>
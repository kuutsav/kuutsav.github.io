[{"categories":null,"contents":"","date":"Jan 26","permalink":"https://iutsav.dev/projects/a_leetcomp/","tags":null,"title":"LeetComp"},{"categories":null,"contents":"Given a set of N objects that support the following two commands:\n Union: Connect two objects. Find/Connected: Is there a path connecting the two obejcts?  For example, consider this set of 10 objects\nAfter few union commands union(2, 3), union(6, 5), union(8, 6), union(10, 8) the state of the system changes to\nWe can query the above system to find if two objects are connected or not like find(0, 1) == False, find(1, 2) == True, find(4, 9) == True, find(8, 1) == False\nTo be formal, we can say that \u0026ldquo;connected to\u0026rdquo; has the following properties:\n Reflexive: a is connected to a. Symmetric: if a is connected to b, then b is conntected to a. Transitive: if a is connected to b and b is conntected to c, then a is connected to c.  Another common terminology in dynamic connectivity problems is Connected components. It refers to the maximal set of objects that are mutually connected. In the above example, the conntected components are {0), {1, 2}, {3}, {4, 5, 7, 9}, {6}, {8}. The union-find algorithms we are going to implement below can help us model objects of many different kinds. Some of the practical examples include:\n Pixels in an image. Computers in a network. Friends in a network. Grid system for path finding problems.  When we are programming the union-find operations, it\u0026rsquo;s convenient to represent the set of objects and their connectivity using a list from 0 to N-1. The overall goal is to design a data structure and algorithm for union-find that is effecient when:\n Number of ojects N can be huge. Number of operations M can be huge. Find and Union queries can be intermixed.     Quick-Find  We will used id[] to store all the obecjts. The index will refer to the objects and the value will indicate the connectivity between two indices.\nFor example id[] = [0, 1, 1, 3, 5, 5, 6, 5, 8, 5]. Here, objects {1, 2} are connected and {4, 5, 7, 9} are connected.\n Find: Check if indices a and b have the same id. Union: To merge indices a and b, change all entries whose id equals id[a] to id[b].  1 2 3 4 5 6 7 8 9 10 11 12 13 14  from collections import defaultdict, Counter import random import time import matplotlib.pyplot as plt import numpy as np def plot(sequences): plt.figure(figsize=(12, 6)) for k, seq, label in sequences: plt.plot(k, seq, label=label) plt.legend() return plt   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  class QuickFind: def __init__(self, n:int=10): self.id = list(range(n)) def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def find(self, a:int, b:int) -\u0026gt; bool: return self.id[a] == self.id[b] def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.id[a] root_of_b = self.id[b] for i, _ in enumerate(self.id): if self.id[i] == root_of_a: self.id[i] = root_of_b def simulate(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) if random.random() \u0026gt; 0.5: st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) else: st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st)   It\u0026rsquo;s easy to observe the worst case time complexity of QuickFind:\n Initialize: O(N) Union: O(N) Find: O(1)  Even though the operations don\u0026rsquo;t look too bad on their own, for a sequence of N union commands on N objects(a very common operation for such problems), this becomes O(N^2).\n1 2 3 4 5 6 7 8 9  timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 11000, 1000): print(i, end=\u0026#34; \u0026#34;) qf = QuickFind(i) st = time.time() qf.simulate() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st)   simulating 100 1100 2100 3100 4100 5100 6100 7100 8100 9100 10100  1 2 3  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]);   1 2 3 4  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]);   An improvement over QuickFind is the QuickUnion algorithm.\n   Quick-Union  Here also we use an array to store the objects, though the interpretation of the values in the array changes, nowid[i] is the parent of i. We are essentially using the array to create a tree like structure. To find the root of any object i, we recursively traverse it\u0026rsquo;s parent till the index is same as the value, i.e. Root of i is id[id[id[\u0026hellip;id[i]\u0026hellip;]]].\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  class QuickUnion: def __init__(self, n:int=10): self.id = list(range(n)) def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def root(self, ix:int) -\u0026gt; int: while ix != self.id[ix]: ix = self.id[ix] return ix def find(self, a:int, b:int) -\u0026gt; bool: return self.root(a) == self.root(b) def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.root(a) root_of_b = self.root(b) self.id[root_of_a] = root_of_b def simulate(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) if random.random() \u0026gt; 0.5: st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) else: st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st) def simulate_worst_case(self) -\u0026gt; None: n = len(self.id) for i in range(n-1): st = time.time() self.union(i, i+1) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) for i in range(n): st = time.time() self.find(i, i) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st)   Here we can observe that the Union and Find operations have a time complexity of O(logN) on average, given that the tree remains balanced. The problem occurs when the tree gets too tall. In the worst case, we can end up with one very tall skinny tree where the time complexity of Find operation becomes O(N). In this case, we again end up with O(N^2) for N finds on N objects.\nSo in the worst case:\n Initialize: O(N) Union: O(N) Find: O(N)     Average Case  1 2 3 4 5 6 7 8 9 10  timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 251000, 1000): if int(str(i)[:2]) % 11 == 0: print(i, end=\u0026#34; \u0026#34;) qu = QuickUnion(i) st = time.time() qu.simulate() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st)   simulating 1100 11100 22100 33100 44100 55100 66100 77100 88100 99100 110100 111100 112100 113100 114100 115100 116100 117100 118100 119100 220100 221100 222100 223100 224100 225100 226100 227100 228100 229100  1 2 3  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]);   1 2 3 4  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]);      Worst case  1 2 3 4 5 6 7 8 9  timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 11000, 1000): print(i, end=\u0026#34; \u0026#34;) qu = QuickUnion(i) st = time.time() qu.simulate_worst_case() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st)   simulating 100 1100 2100 3100 4100 5100 6100 7100 8100 9100 10100  1 2 3  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]);   1 2 3 4  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]);      Quick-Union-Weighted  The QuickUnion can easily be improved by trying to keep the trees balanced as the number of operations grow.\nThis can be done by keeping track of the size of the trees and using it for each union(a, b) operation. Now, instead of just changing the root of a to root of b, we compare the size of tree at a and the tree at b to decide whose root changes.\nIn the worst case now:\n Initialize: O(N) Union: O(logN) Find: O(logN)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60  class QuickUnionWeighted: def __init__(self, n:int=10): self.id = list(range(n)) self._sizes = [1] * n def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def root(self, ix:int) -\u0026gt; int: while ix != self.id[ix]: ix = self.id[ix] return ix def find(self, a:int, b:int) -\u0026gt; bool: return self.root(a) == self.root(b) def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.root(a) root_of_b = self.root(b) if root_of_a == root_of_b: return if self._sizes[root_of_a] \u0026gt; self._sizes[root_of_b]: self._sizes[root_of_a] += self._sizes[root_of_b] self.id[root_of_b] = root_of_a else: self._sizes[root_of_b] += self._sizes[root_of_a] self.id[root_of_a] = root_of_b def simulate(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) if random.random() \u0026gt; 0.5: st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) else: st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st) def simulate_worst_case(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st)   With balanced trees, for N Find operations over N objects, our worst time complexity should be O(NlogN).\n1 2 3 4 5 6 7 8 9 10  timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 251000, 1000): if int(str(i)[:2]) % 11 == 0: print(i, end=\u0026#34; \u0026#34;) quw = QuickUnionWeighted(i) st = time.time() quw.simulate() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st)   simulating 1100 11100 22100 33100 44100 55100 66100 77100 88100 99100 110100 111100 112100 113100 114100 115100 116100 117100 118100 119100 220100 221100 222100 223100 224100 225100 226100 227100 228100 229100  1 2 3  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]);   1 2 3 4  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]);   1 2 3 4 5 6 7 8 9 10  timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 251000, 1000): if int(str(i)[:2]) % 11 == 0: print(i, end=\u0026#34; \u0026#34;) quw = QuickUnionWeighted(i) st = time.time() quw.simulate_worst_case() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st)   simulating 1100 11100 22100 33100 44100 55100 66100 77100 88100 99100 110100 111100 112100 113100 114100 115100 116100 117100 118100 119100 220100 221100 222100 223100 224100 225100 226100 227100 228100 229100  1 2 3  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]);   1 2 3 4  plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]);      Percolation  Percolation is a model for many physical systems. The system can be represented in the following way:\n N-by-N grid of sites. Each site is open with probability p (or blocked with probability 1-p). System percolates if the top and bottom are connected by open sites.  One example can be water flowing through a block of bricks with open sites indicating porus material. The likelihood of percolation depends on the site vacancy probability p. We will use the QuickUnionWeighted algorithm to find the threshold for p where the liklehood of percolation changes suddenly from 0 to 1.\nWe will run monte carlo simulation on a system:\n Which starts with all sites blocked. In each step, we randomly open a site and check if the system percolates. We keep repeating the last step till the system percolates. We estimate p as (# open sites) / (N * N). Repeat the above steps M(1000x) times.  If there is such a threshold for p where \u0026ldquo;the liklehood of percolation changes suddenly from 0 to 1\u0026rdquo;, then a running mean of all the p over multiple simulations should give us that number according to the law of large numbers.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94  class PercolationModel: def __init__(self, n:int=10): self.id = list(range(n)) self._sizes = [1] * n self._open = [0] * n self._ixs = list(range(n)) self.n = int(np.sqrt(n)) self.nxn = n def reset(self) -\u0026gt; None: self.id = list(range(self.nxn)) self._sizes = [1] * self.nxn self._open = [0] * self.nxn self._ixs = list(range(self.nxn)) def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def root(self, ix:int) -\u0026gt; int: while ix != self.id[ix]: ix = self.id[ix] return ix def find(self, a:int, b:int) -\u0026gt; bool: return self.root(a) == self.root(b) def is_valid_ix(self, r:int, c:int) -\u0026gt; bool: return (r \u0026gt; -1 and r \u0026lt; self.n) and (c \u0026gt; -1 and c \u0026lt; self.n) def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.root(a) root_of_b = self.root(b) if self._sizes[root_of_a] \u0026gt; self._sizes[root_of_b]: self._sizes[root_of_a] += self._sizes[root_of_b] self.id[root_of_b] = root_of_a else: self._sizes[root_of_b] += self._sizes[root_of_a] self.id[root_of_a] = root_of_b def open_site(self) -\u0026gt; None: ix = random.sample(self._ixs, 1)[0] self._ixs.remove(ix) self._open[ix] = 1 r, c = ix // self.n, ix % self.n if self.is_valid_ix(r-1, c) and self._open[ix-self.n]: # up self.union(ix, ix-self.n) if self.is_valid_ix(r, c-1) and self._open[ix-1]: # left self.union(ix, ix-1) if self.is_valid_ix(r, c+1) and self._open[ix+1]: # right self.union(ix, ix+1) if self.is_valid_ix(r+1, c) and self._open[ix+self.n]: # down self.union(ix, ix+self.n) def percolates(self) -\u0026gt; bool: last_r = self.n * (self.n-1) for c1 in range(self.n): for c2 in range(self.n): if self.find(c1, last_r+c2): return True return False def simulate(self, n_sims:int=1000, till_p=None) -\u0026gt; (float, list): p = []; for i in range(n_sims): self.reset() n_open_sites = 0 if till_p: while (n_open_sites / self.nxn) \u0026lt; till_p: self.open_site() n_open_sites += 1 p.append(n_open_sites / self.nxn) else: while not self.percolates(): self.open_site() n_open_sites += 1 p.append(n_open_sites / self.nxn) return np.mean(p), [np.mean(p[:i+1]) for i in range(n_sims)] def display(self) -\u0026gt; None: coords, colors = [], []; for r in range(pm.n): for c in range(pm.n): coords.append((r, c)) if pm._open[pm.n*r+c]: colors.append(\u0026#34;lightblue\u0026#34;) else: colors.append(\u0026#34;black\u0026#34;) plt.figure(figsize=(5, 5)) plt.scatter([c[0] for c in coords], [c[1] for c in coords], c=colors, s=740, marker=\u0026#34;s\u0026#34;) plt.xticks([]); plt.yticks([]);   Let\u0026rsquo;s see how the system looks at different values of p, for different 10-by-10 grids.\n1 2 3  pm = PercolationModel(100) p, _ = pm.simulate(till_p=0.2) p   0.20000000000000004  1  pm.display()   1 2 3  pm = PercolationModel(100) p, _ = pm.simulate(till_p=0.8) p   0.8000000000000002  1  pm.display()   Let\u0026rsquo;s find the p threshold.\n1 2 3  pm = PercolationModel(100) p, ps = pm.simulate(n_sims=2000) p   0.59062  1  pm.display()   1 2  plt.figure(figsize=(12, 6)) plt.plot(list(range(len(ps))), ps);   1 2 3  pm = PercolationModel(100) p, ps = pm.simulate(n_sims=5000) p   0.590384  1  pm.display()   1 2  plt.figure(figsize=(12, 6)) plt.plot(list(range(len(ps))), ps);      References  [1] https://www.coursera.org/learn/algorithms-part1/home/week/1.\n","date":"Apr 07","permalink":"https://iutsav.dev/posts/dynamic_connectivity_and_percolation/","tags":["python","dynamic connectivity","algorithms"],"title":"Dynamic Connectivity and Percolation"},{"categories":null,"contents":"Python provides a variety of sequences; understanding these builtin sequences saves us from reinventing the wheel. We can also aspire to create APIs that support existing and user created sequence types. Python offers two types of sequences:\n Container sequences: These hold items of different types, for example list, tuple, etc. Flat sequences: These hold items of the same type, for example str, bytes, etc.  A container sequence holds references to objects where as a flat sequence contains the value of the contents in it\u0026rsquo;s own memory. To understand the above point clearly, we need to understand how python stores objects.\nEvery python object has a header with metadata. For example, a float has:\n ob_refcnt: Object\u0026rsquo;s reference count. ob_type: Object\u0026rsquo;s type. ob_fval: Object\u0026rsquo;s value.  Each of these fields take 8 bytes in a 64-bit built. So an array.array of floats actually holds the raw values of the floats whereas a list of floats has several objects - the list iteself and reference to each float object. Another way the sequences can be classified is based on it\u0026rsquo;s mutability:\n Mutable sequences: list, array.array, collections.deque, etc. Immutable sequences: tuple, str, bytes, etc.     List Comprehensions  List comprehensions or listcomps are used to create a new list from a list by transforming or/and filtering the data in the original list.\nListcomps can be created using mulltiple loops over multiple iterators as well, the key idea is to make sure that the script is readable and the transformation is obvious to the end user. It\u0026rsquo;s very easy to abuse listcomps to write incomprehensible code. We should stick to plain old for loops in such cases. Listcomps are also faster than map and filter.\n1 2  %%timeit even_squares = [x**2 for x in range(100) if x % 2 == 0]   14.6 µs ± 47.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  1  print(even_squares)   (0, 4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604)  1 2  %%timeit even_squares = list(filter(lambda x: x % 2 == 0, map(lambda x: x**2, range(100))))   33.6 µs ± 867 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  1  print(even_squares)   (0, 4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604)     Generator Expressions  To initialize sequences of types other than list, like arrays, tuples, etc., we can use genexp as they save memory by yielding one item at a time using the iterator protocol. A listcomp creates a whole list to feed into another constructor.\n1 2  even_squares = tuple(x**2 for x in range(100) if x % 2 == 0) print(even_squares)   (0, 4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604)     Tuples  Beyond using tuples as \u0026ldquo;immutable lists\u0026rdquo;, they can also be thought of as a system of records. Tuples along with \u0026ldquo;unpacking\u0026rdquo; make for very readable way to access elements from a sequence by avoiding indexing.\n1 2 3  for country, capital in [(\u0026#34;India\u0026#34;, \u0026#34;New Delhi\u0026#34;), (\u0026#34;Afganistan\u0026#34;, \u0026#34;Kabul\u0026#34;), (\u0026#34;Canada\u0026#34;, \u0026#34;Ottawa\u0026#34;)]: print(f\u0026#34;{country}/{capital}\u0026#34;)   India/New Delhi Afganistan/Kabul Canada/Ottawa     Immutaiblity of Tuples  The fact that tuples are immutable has many benefits:\n We know that the value of a tuple object will never change. Tuples are faster than lists as they have less overheads.  Tuple are faster than lists because:\n   Tuples can be constant folded  1 2 3  from dis import dis dis(compile(\u0026#34;(10, \u0026#39;abc\u0026#39;)\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;eval\u0026#34;))    1 0 LOAD_CONST 0 ((10, 'abc')) 2 RETURN_VALUE  1  dis(compile(\u0026#34;[10, \u0026#39;abc\u0026#39;]\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;eval\u0026#34;))    1 0 LOAD_CONST 0 (10) 2 LOAD_CONST 1 ('abc') 4 BUILD_LIST 2 6 RETURN_VALUE     Tuples do not need to be copied  1 2 3  a = (10, 20) b = tuple(a) a is b   True  1 2 3  a = [10, 20] b = list(a) a is b   False     Tuples do not over-allocate  1 2 3  import sys sys.getsizeof(tuple(iter(range(10))))   120  1  sys.getsizeof(list(iter(range(10))))   192  Here is the comment from Objects/listobject.c that explains what lists are doing:\n* This over-allocates proportional to the list size, making room * for additional growth. The over-allocation is mild, but is * enough to give linear-time amortized behavior over a long * sequence of appends() in the presence of a poorly-performing * system realloc(). * The growth pattern is: 0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ... * Note: new_allocated won't overflow because the largest possible value * is PY_SSIZE_T_MAX * (9 / 8) + 6 which always fits in a size_t. Coming back to the immutability aspect of Tuples, the fact that tuples are container style sequences, they can hold mutable objects.\n1 2 3  a = (10, \u0026#34;alpha\u0026#34;, [1, 2]) b = (10, \u0026#34;alpha\u0026#34;, [1, 2]) a == b   True  1 2  b[-1].append(99) a == b   False  1  b   (10, 'alpha', [1, 2, 99])  Examples like this can be a source of bugs. It\u0026rsquo;s always a good idea to avoid mutable objects in a tuple. A tuple without mutable objects can be used as a key for a dict. This can be verified using the hash method.\n1  hash((10, \u0026#34;alpha\u0026#34;, 1, 2))   1614661222696736202  1  hash((10, \u0026#34;alpha\u0026#34;, [1, 2]))   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-58-6818f30e4402\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 hash((10, \u0026quot;alpha\u0026quot;, [1, 2])) TypeError: unhashable type: 'list'     Unpacking sequences and iterables  Unpacking helps avoid indexing to extract items from sequences. It works with any iterable object, even if the object doesn\u0026rsquo;t support index notation [].\n1 2  country, capital = (\u0026#34;India\u0026#34;, \u0026#34;New Delhi\u0026#34;) country   'India'  1  capital   'New Delhi'  An elegant way to use unpacking is to swap values without using a temporary variable.\n1 2 3 4  a = 10 b = 20 a, b = b, a a, b   (20, 10)  We can use * to grab excess items.\n1 2  a, b, *rest = range(5) a, b, rest   (0, 1, [2, 3, 4])  1 2  a, *rest, b = range(10) a, rest, b   (0, [1, 2, 3, 4, 5, 6, 7, 8], 9)  Unpacking works with nested structures as well.\n1 2 3 4 5  shopping_cart = [(1, \u0026#34;apple\u0026#34;, (3.2, 10)), (2, \u0026#34;milk\u0026#34;, (3, 20))] print(f\u0026#34;{\u0026#39;item\u0026#39;:\u0026lt;10}| {\u0026#39;cost\u0026#39;:\u0026gt;4}{\u0026#39;qnt\u0026#39;:\u0026gt;4}{\u0026#39;total\u0026#39;:\u0026gt;5}\\n{\u0026#39;-\u0026#39;*28}\u0026#34;) for _, item, (cost, qnt) in shopping_cart: print(f\u0026#34;{item:\u0026lt;10}| {cost:\u0026gt;4}{qnt:\u0026gt;4}{cost*qnt:\u0026gt;5}\u0026#34;)   item | cost qnt total ---------------------------- apple | 3.2 10 32.0 milk | 3 20 60     Slicing  Slices in python excldue the last item. This convention is common across many languages. This feature has many convenient side effects:\n Length of range(3) and my_list[:3] is the same. It\u0026rsquo;s easy to compute the length of a slice when start and stop are given using stop - start. It\u0026rsquo;s easy to split a sequence at an index x using my_list[:x] and my_list[x:]  1 2  my_list = list(range(10)) my_list[:3], my_list[3:]   ([0, 1, 2], [3, 4, 5, 6, 7, 8, 9])  One uncommon use of slices is to create constants to make the code more readable. By assigning names to slices we can be more explicit instead of needing the user to figure out what the indexes mean.\n1 2 3 4 5 6 7 8 9 10 11 12  invoice = \u0026#34;\u0026#34;\u0026#34; 1 apple $3.2 10 2 banana $1.5. 5 \u0026#34;\u0026#34;\u0026#34; SKU_ID = slice(0, 2) SKU_NAME = slice(3, 9) PRICE = slice(10, 14) QUANTITY = slice(15, 17) for sku_line in invoice.split(\u0026#34;\\n\u0026#34;)[1:-1]: print(sku_line[SKU_ID], sku_line[SKU_NAME], sku_line[PRICE], sku_line[QUANTITY])   1 apple $3.2 10 2 banana $1.5 5  The [] operator can also take multpile indexes and slices separated by commas. This pattern is common in external libraries like numpy and pandas where this notation is used to indicate index/slice across rows and columns.\n1 2 3 4  import numpy as np arr = np.array([[1, 2, 3], [4, 5, 6]]) arr   array([[1, 2, 3], [4, 5, 6]])  1  arr[:, 1:] # all rows, columns 2 and 3   array([[2, 3], [5, 6]])  Slices can also be used on the left-hand side of an assignment operation or as the target of a del operation.\n1 2  my_list = list(range(10)) my_list   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  1 2  my_list[3:5] = [30, 40] my_list   [0, 1, 2, 30, 40, 5, 6, 7, 8, 9]  1 2  del my_list[3:5] my_list   [0, 1, 2, 5, 6, 7, 8, 9]     Using * with Sequences  To concatenate multiple copies of the same sequence, we multiply it by an integer which creates a new sequence.\n1 2 3  my_list = [1, 2, 3] new_list = my_list * 3 new_list   [1, 2, 3, 1, 2, 3, 1, 2, 3]  1  id(my_list), id(new_list)   (140325040574144, 140325040580864)  Using * to multiply sequences with mutable objects can lead to weird behavior due to the fact that we end of referring to the same object.\n1 2  weird_board = [[\u0026#34;_\u0026#34;] * 3] * 3 weird_board   [['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]  1  weird_board[0] is weird_board[1]   True  Looking at the above(both referring to the same object), the below result should not be a surprise.\n1 2  weird_board[0][2] = \u0026#34;X\u0026#34; weird_board   [['_', '_', 'X'], ['_', '_', 'X'], ['_', '_', 'X']]     Augmented Assignment with Sequences  The augmented assignment += works using the special method __iadd__. If __iadd__ is not implemented, Python falls back to calling __add__.\n1 2 3  my_list = [1, 2, 3] my_list += [4, 5] my_list   [1, 2, 3, 4, 5]  __iadd__ works similar to .extend(). If my_list does not implement __iadd__, a+=b is the same as a=a+b.\n   Usage  Although it\u0026rsquo;s highlighted everywhere that lists are container type sequences that can hold objects of different types, in practice we typically store objects of similar type in a list so that we can apply some common operation on them(i.e. they should all \u0026ldquo;quack\u0026rdquo; whether or not they are 100% ducks).\nTuples being used as records can be used to store objects of different types in practice.\n1 2  my_list = [1, 2, \u0026#34;30\u0026#34;, 4, \u0026#34;5\u0026#34;] my_list   [1, 2, '30', 4, '5']  1  sorted(my_list)   --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-135-cb83e3f42d3f\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 sorted(my_list) TypeError: '\u0026lt;' not supported between instances of 'str' and 'int'  Most of the languages need us to provide a cmp(a, b) method to make comparisons to be able to sort a sequence. In python, using key argument in list.sort, max, min or sorted methods removes the need for this cmp() method. Python does compare keys but that is done in optimized C code and not using a method provided by us.\n1  sorted(my_list, key=int)   [1, 2, 4, '5', '30']  1  sorted(my_list, key=str)   [1, 2, '30', 4, '5']     References  [1] Luciano Ramalho. \u0026ldquo;Fluent Python\u0026rdquo;.\n","date":"Apr 05","permalink":"https://iutsav.dev/posts/python_notes_2_sequences/","tags":["python","sequences"],"title":"Python Notes, Part 2 - Sequences"},{"categories":null,"contents":"One of the best things about python is it\u0026rsquo;s consistency. Once someone has spent enough time with python and it\u0026rsquo;s ecosystem, they tend to adapt easily to new tools, modules as long as they are written in a pythonic way.\nWhat is considered pythonic is a very subjective matter, but we can always look at the \u0026ldquo;Zen of Python\u0026rdquo; for some guidance.\n1  import this   The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those!     Special Methods (dunder methods)  The heart of what makes an object pythonic is the Python Data Model. Python Data Model provides apis that help us create consistent behavior for our python objects. We achieve this consistency by adding special methods called dunder methods(or double underscore methods) to our objects.\nBefore looking into dunder methods, lets look at a simple object to demonstrate a point about python objects and the consistency we keep talking about.\nTypically in Object Oriented Design we define a class which encapsulates some data and the associated methods that act on the data.\nSuppose we have an object which internally maintains a collection and we want to know the length of the collection, typically the OOO design will go something like\nMyObject.len() or MyObject.size() ... whether it\u0026rsquo;s len(), length(), size() or something else is the choice of the designer of this API and the users need to read the documentation/specification before using the object.\nIn python, we do this by calling len(MyObject). Now this is not a typical OOP behaviour, but this is what is magical about python. Under the hood, len() refers to the __len__ dunder method of the object. By adding these special methods to our class definition, we can provide consistent behaviour to our objects so that new users can start using them easily and in a predictable manner(as long as they expect this behaviour).\nPython has many more dunder methods. These methods provide a framework for building almost everything in python, be it iterators, collections, string representations, classes, coroutines, etc.\nLet\u0026rsquo;s look at more dunder methods using the following example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  from dataclasses import dataclass import random @dataclass class DiePair: die1: int die2: int class TwoDieRoll: def __init__(self): self._pairs = [DiePair(die1=i, die2=j) for i in range(1, 7) for j in range(1, 7)] def __getitem__(self, ix: int): return self._pairs[ix] def __len__(self): return len(self._pairs) def roll(self): return random.choice(self._pairs)   1 2 3  two_dies = TwoDieRoll() len(two_dies)   36  1  two_dies[10]   DiePair(die1=2, die2=5)  Beyond indexing, by adding __getitem__ we get to access to other features like slicing\n1  two_dies[:5]   [DiePair(die1=1, die2=1), DiePair(die1=1, die2=2), DiePair(die1=1, die2=3), DiePair(die1=1, die2=4), DiePair(die1=1, die2=5)]  1  two_dies[-5:]   [DiePair(die1=6, die2=2), DiePair(die1=6, die2=3), DiePair(die1=6, die2=4), DiePair(die1=6, die2=5), DiePair(die1=6, die2=6)]  Our object two_dies is also an iterable now\n1 2  for die_pair in two_dies: print(die_pair)   DiePair(die1=1, die2=1) DiePair(die1=1, die2=2) DiePair(die1=1, die2=3) DiePair(die1=1, die2=4) DiePair(die1=1, die2=5) DiePair(die1=1, die2=6) DiePair(die1=2, die2=1) DiePair(die1=2, die2=2) DiePair(die1=2, die2=3) ...  Iteration is often implicit, for example\n1  DiePair(1, 5) in two_dies   True  1  DiePair(1, 8) in two_dies   False  Both the above example work without the __contains__ dunder method in our class. In it\u0026rsquo;s absence, the in operator does a sequential scan.\nWe can use built in modules to add methods like roll() to our class.\n1  two_dies.roll()   DiePair(die1=3, die2=1)  We can also sort the dies in reverse order based on die2.\n1  sorted(two_dies, key=lambda die: die.die2, reverse=True)   [DiePair(die1=1, die2=6), DiePair(die1=2, die2=6), DiePair(die1=3, die2=6), DiePair(die1=4, die2=6), DiePair(die1=5, die2=6), DiePair(die1=6, die2=6), DiePair(die1=1, die2=5), DiePair(die1=2, die2=5), DiePair(die1=3, die2=5), ...]  All this is possible because of __getitem__ which makes our object behave like a python collection.\nDunder methods are not called directly by the user, but internally by the interpreter, except in special cases like metaprogramming or in the __init__ of a class to initialize the base class it\u0026rsquo;s deriving from.\nThat does not mean we can\u0026rsquo;t invoke them explicitly.\n1  two_dies.__len__()   36     More uses of dunder methods  Dunder methods have lots of other uses as well. For example\n Emulating numeric types(operator overloading) String representation for objects     Emulating Numeric Types  Let\u0026rsquo;s create a class that allows us to create vectors we use in physics or to represent data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import math class Vector: def __init__(self, x: int, y: int): self.x = x self.y = y def __repr__(self): return f\u0026#34;Vector({self.x}, {self.y})\u0026#34; def __add__(self, other_vector): return Vector(self.x + other_vector.x, self.y + other_vector.y) def __mul__(self, scalar: int): return Vector(scalar * self.x, scalar * self.y) def __abs__(self): return math.hypot(self.x, self.y)   By adding __add__ and __mul__ special methods, we can start adding two vectors and multiplying scalars to our vectors like we expect.\n1 2  vector1 = Vector(1, 2) vector2 = Vector(10, 3)   1  vector1 + vector2   Vector(11, 5)  1  vector2 * 3   Vector(30, 9)  1  abs(Vector(3, 4))   5.0     String representation of objects  The __repr__ method allows us to create a string representation for the object. This is useful for debugging/logging purposes. If possible, it should return a string that describes exactly how the object is created.\n1  vector1   Vector(1, 2)  1  vector2   Vector(10, 3)  We also have a __str__ method that returns a string describing the object. Typically this is a description of the object for the end user.\nWhen the __repr__ is descriptive enough, the __str__ method can be avoided as it defaults to __repr__.\nFrom stackoverflow:\n My rule of thumb: repr is for developers, str is for customers.\n    Collection API  The UML diagram shows the three essential interface every collection should implement\n Iterable Sized Container  Three very important specializations of collection are\n Sequence: formalizing the interface of built-ins like list and str; Mapping: implemented by dict, collections.defaultdict, etc.; Set: the interface of the set and frozenset built-in types.  For example, let\u0026rsquo;s create a list\n1 2  my_list = [1, 2, 3] dir(my_list)   ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']  The dir() method shows all the methods and special methods available for an object. Here we can see all the essential interfacs as well as others. The built in python collections implement these special methods for us.\nLet\u0026rsquo;s look at a set now.\n1 2  my_set = {1, 2, 3} dir(my_set)   ['__and__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__iand__', '__init__', '__init_subclass__', '__ior__', '__isub__', '__iter__', '__ixor__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__rand__', '__reduce__', '__reduce_ex__', '__repr__', '__ror__', '__rsub__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__xor__', 'add', 'clear', 'copy', 'difference', 'difference_update', 'discard', 'intersection', 'intersection_update', 'isdisjoint', 'issubset', 'issuperset', 'pop', 'remove', 'symmetric_difference', 'symmetric_difference_update', 'union', 'update']  1  {1, 2, 3} - {1, 2}   {3}  We are able to do the above operation because of the special method __sub__ that\u0026rsquo;s been provided to us by default.\n   References  [1] Luciano Ramalho. \u0026ldquo;Fluent Python\u0026rdquo;.\n[2] https://docs.python.org/3/reference/datamodel.html.\n","date":"Mar 05","permalink":"https://iutsav.dev/posts/python_notes_1_datamodel/","tags":["python","data model"],"title":"Python Notes, Part 1 - Data Model"},{"categories":null,"contents":"We will use FastAPI to serve our trained models behind a REST endpoint. FastAPI is a web framework built on top of Starlette. Scripts related to the APIs are located at MLOps/app.\nThe directory structure is:\n1 2 3 4 5 6 7  ├── application.py # FastAPI app and launcher uvicorn.run ├── models # pydantic model for reponse validation │ └── predict.py # pydantic model for predict endpoint └── routes # to maintain larger applications ├── endpoints │ └── predict.py # predict endpoint └── router.py # APIRouter module to maintain larger apps   Though we have used the APIRouter module here, it\u0026rsquo;s really needed when we have lots of API endpoints in a larger app.\nLook at https://fastapi.tiangolo.com/tutorial/bigger-applications/.\n   Model Artifacts  To use the model to get predictions, we need to load all the model related artifacts.\nWe load the trained model directly using the mlflow.sklearn module. Here we load the version number 1 of our sk-learn-naive-bayes-clf-model model.\n1 2 3 4 5 6 7 8 9 10 11  # manually pick the model version from trained models sk_model = mlflow.sklearn.load_model(model_uri=\u0026#34;models:/sk-learn-naive-bayes-clf-model/1\u0026#34;) # mlflow does not store data manipulation routines like label encoding # we need to manage the LabelEncoder and TfidfVectorizer ourselves with open(BASE_DIR / \u0026#34;artifacts/target_encoder.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: target_encoder = pickle.load(f) with open(BASE_DIR / \u0026#34;artifacts/vectorizer.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: vectorizer = pickle.load(f) logger.info(\u0026#34;Loaded model artifacts\u0026#34;)   Beyond the trained classifier, we also need the TfidfVectorizer to vectorize the text and LabelEncoder to map the predictions to actual labels.\nThese artifacts are not saved/managed by MLflow as it only mangages the artifacts realted to the ML algorithm. #\n Exclude certain preprocessing \u0026amp; feature manipulation estimators from patching. These estimators represent data manipulation routines (e.g., normalization, label encoding) rather than ML algorithms. Accordingly, we should not create MLflow runs and log parameters / metrics for these routines, unless they are captured as part of an ML pipeline (via sklearn.pipeline.Pipeline).\n 1 2 3 4 5 6 7 8 9 10 11  # manually pick the model version from trained models sk_model = mlflow.sklearn.load_model(model_uri=\u0026#34;models:/sk-learn-naive-bayes-clf-model/1\u0026#34;) # mlflow does not store data manipulation routines like label encoding # we need to manage the TfidfVectorizer and TfidfVectorizer ourselves with open(BASE_DIR / \u0026#34;artifacts/target_encoder.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: target_encoder = pickle.load(f) with open(BASE_DIR / \u0026#34;artifacts/vectorizer.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: vectorizer = pickle.load(f) logger.info(\u0026#34;Loaded model artifacts\u0026#34;)      Predict Endpoint #  Let\u0026rsquo;s look at the /predict endpoint.\n1 2 3 4 5 6 7 8 9 10  @router.post(\u0026#34;/predict\u0026#34;) async def predit(text: str) -\u0026gt; PredictResponseModel: logger.info(f\u0026#34;Received text for prediction: {text}\u0026#34;) processed_text_list = preprocess_text([text]) x = vectorizer.transform(processed_text_list) pred = sk_model.predict_proba(x) mapped_pred = dict(zip(target_encoder.classes_, pred[0])) logger.info(f\u0026#34;Prediction: {mapped_pred}\u0026#34;) return PredictResponseModel(preds=mapped_pred).preds   Here, we preprocess the text using preprocess_text(), vectorize it using vectorizer.transform() and finally generate predictions using the classifier sk_model.predict_proba().\nWe then map the probabilities to the actual labels(target_encoder.classes_) and return the predictions by wrapping around the PredictResponseModel pydantic model for data validation.\nThe API can be accessed at http://127.0.0.1:8010/docs.\n   ElasticAPM  We have also added ElasticAPM as a middleware for monitoring our FastAPI application. #\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  import uvicorn from elasticapm.contrib.starlette import ElasticAPM, make_apm_client from fastapi import FastAPI from loguru import logger from mlops.app.routes.router import api_router def get_fastapi_application() -\u0026gt; FastAPI: application = FastAPI(title=\u0026#34;MLOps\u0026#34;) application.add_middleware( ElasticAPM, client=make_apm_client({\u0026#34;SERVICE_NAME\u0026#34;: \u0026#34;MLOps Example\u0026#34;}) ) application.include_router(api_router) return application app = get_fastapi_application() if __name__ == \u0026#34;__main__\u0026#34;: logger.info(\u0026#34;*** Starting Prediction Server ***\u0026#34;) uvicorn.run(app, host=\u0026#34;127.0.0.1\u0026#34;, port=8010)   Once we start using the /predict endpoint, we can head over to http://localhost:5601/ to look at the app related metrics like Latency, Throughput etc, and system level metrics like CPU usage and System memory usage.\nUnder the hood, it uses Elastic Search, Kibana and an APM server launched using the docker-compose-monitoring.yaml during the setup.\nThe dashboard also provides a trace for each request.\n","date":"Nov 19","permalink":"https://iutsav.dev/posts/mlops_template_4_fastapi_elasticapm/","tags":["elasticapm","fastapi"],"title":"MLOps Template, Part 4 - FastAPI + ElasticAPM"},{"categories":null,"contents":"In the last part, we saw our pipeline at the end. Steps 1-5 deal with data manipulation. We train our model in step 6 and that\u0026rsquo;s where MLflow comes into picture.\n1 2 3 4 5 6 7 8 9 10 11 12 13  @pipeline def ml_pipeline(): # 1. fetch training data texts, target = get_training_dataset() # 2. minimal text preprocessing # 3. tfidf vectorization vectorizer, X = get_vectorizer_and_features(preprocess_text(texts)) # 4. target encoding target_encoder, encoded_target = get_targetencoder_and_encoded_targets(target) # 5. train test split X_train, X_test, y_train, y_test = train_test_split(X, encoded_target) # 6. model training, validation, registry, artifact storage train_clf(X_train, X_test, y_train, y_test)      MLflow #   MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It tackles four primary functions:\nTracking experiments to record and compare parameters and results (MLflow Tracking).\nPackaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production (MLflow Projects).\nManaging and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\nProviding a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\n For our usecase, we mainly focus on Mlflow Tracking and Mlflow Registry. train_clf() internally calls train_and_validate_clf() located at mlops/ml_workflow/naive_bayes_clf.py.\nThis script has as all the MLflow components and we will look at those in detail.\n   Tracking #  1 2 3 4 5 6 7 8 9 10 11 12  # setting env vars for minio artifact storage set_env_vars() mlflow.set_tracking_uri(os.getenv(\u0026#34;MLFLOW_TRACKING_URI\u0026#34;)) # creates a new mlflow experiment MLFLOW_EXPERIMENT_NAME if it doesn\u0026#39;t exist exps = [exp.name for exp in mlflow.tracking.MlflowClient().list_experiments()] if not os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;) in exps: mlflow.create_experiment( os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;), artifact_location=os.getenv(\u0026#34;MLFLOW_ARTIFACT_LOCATION\u0026#34;), )    MLflow runs can be recorded to local files, to a SQLAlchemy compatible database, or remotely to a tracking server. By default, the MLflow Python API logs runs locally to files in an mlruns directory wherever you ran your program. You can then run mlflow ui to see the logged runs.\nTo log runs remotely, set the MLFLOW_TRACKING_URI environment variable to a tracking server’s URI or call mlflow.set_tracking_uri().\nThere are different kinds of remote tracking URIs:\n Local file path (specified as file:/my/local/dir), where data is just directly stored locally. Database encoded as \u0026lt;dialect\u0026gt;+\u0026lt;driver\u0026gt;://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt;. MLflow supports the dialects mysql, mssql, sqlite, and postgresql. For more details, see SQLAlchemy database uri. HTTP server (specified as https://my-server:5000), which is a server hosting an MLflow tracking server. Databricks workspace (specified as databricks or as databricks://\u0026lt;profileName\u0026gt;, a Databricks CLI profile. Refer to Access the MLflow tracking server from outside Databricks [AWS] [Azure], or the quickstart to easily get started with hosted MLflow on Databricks Community Edition.   We use sqlite for tracking. MLFLOW_TRACKING_URI=sqlite:///mlflow.db. This file stays at the root directory of the project.\n You can optionally organize runs into experiments, which group together runs for a specific task. You can create an experiment using the mlflow experiments CLI, with mlflow.create_experiment(), or using the corresponding REST parameters. The MLflow API and UI let you create and search for experiments.\n 1 2 3 4 5 6 7 8 9 10 11 12  # setting env vars for minio artifact storage set_env_vars() mlflow.set_tracking_uri(os.getenv(\u0026#34;MLFLOW_TRACKING_URI\u0026#34;)) # creates a new mlflow experiment MLFLOW_EXPERIMENT_NAME if it doesn\u0026#39;t exist exps = [exp.name for exp in mlflow.tracking.MlflowClient().list_experiments()] if not os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;) in exps: mlflow.create_experiment( os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;), artifact_location=os.getenv(\u0026#34;MLFLOW_ARTIFACT_LOCATION\u0026#34;), )   First time we execute the pipeline, experiment MLFLOW_EXPERIMENT_NAME=MLOps is created and is used for subsequent runs.\n1 2 3 4 5 6 7 8  def train_and_validate_clf( X_train: np.array, X_test: np.array, y_train: np.array, y_test: np.array ) -\u0026gt; str: mlflow.set_experiment(os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;)) with mlflow.start_run(run_name=\u0026#34;NAIVE_BAYES_CLF\u0026#34;): # ignoring rest of the script pass      Registry #  Under an experiment, we can log metrics, log model parameters, save and version model artifacts. These steps come under the purview of model-registry. Here, we log\n classifier parameter alpha classification metrics precision, recall and f1_score  We also save and version the model using mlflow.sklearn.log_model. Each subsequent run of the pipeline with the same artifact_path and registered_model_name increases the model version by 1 in the registry.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  def train_and_validate_clf( X_train: np.array, X_test: np.array, y_train: np.array, y_test: np.array ) -\u0026gt; str: mlflow.set_experiment(os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;)) with mlflow.start_run(run_name=\u0026#34;NAIVE_BAYES_CLF\u0026#34;): clf = MultinomialNB() mlflow.log_param(\u0026#34;alpha\u0026#34;, clf.get_params()[\u0026#34;alpha\u0026#34;]) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) scores = precision_recall_fscore_support(y_test, y_pred, average=\u0026#34;weighted\u0026#34;) mlflow.log_metric(\u0026#34;precision\u0026#34;, scores[0]) mlflow.log_metric(\u0026#34;recall\u0026#34;, scores[1]) mlflow.log_metric(\u0026#34;f1_score\u0026#34;, scores[2]) mlflow.sklearn.log_model( sk_model=clf, artifact_path=\u0026#34;naive-bayes-model\u0026#34;, registered_model_name=\u0026#34;sk-learn-naive-bayes-clf-model\u0026#34;, ) return classification_report(y_test, y_pred)   Heading over to the experiment section at http://127.0.0.1:5000, we can see the registry entry for each of our runs, along with the logged metrics and model parameters.\nFor each run, we can also look at the artifact location.\nWhen we created the experiment, we speficified artifact_location=os.getenv(\u0026quot;MLFLOW_ARTIFACT_LOCATION\u0026quot;) where MLFLOW_ARTIFACT_LOCATION=s3://mlflow. Internally, this stores the model artifacts locally at the directory minio_data we created during the setup.\nWe can head over to http://127.0.0.1:9001/buckets/mlflow/browse to look at the saved model artifacts.\nIn the next part, we will look at serving the trained model being a REST endpoint.\n","date":"Nov 05","permalink":"https://iutsav.dev/posts/mlops_template_3_mlflow_minio/","tags":["minio","mlflow"],"title":"MLOps Template, Part 3 - MLflow + MinIO"},{"categories":null,"contents":"Here we will cover our small pipeline written in dagster.\n Dagster is a data orchestrator for machine learning, analytics, and ETL\n Dagster is a second generation data orchestrator that focues on being data-driven rather than task-driven(like Airflow).\n   Solids and Pipelines #   Dagster\u0026rsquo;s core abstractions are solids and pipelines.\nSolids are individual units of computation that we wire together to form pipelines. By default, all solids in a pipeline execute in the same process. In production environments, Dagster is usually configured so that each solid executes in its own process.\n    Dataset  Our pipeline will operate on Question Type Classification. This data helps to classify the given Questions into respective categories based on what type of answer it expects such as a numerical answer or a text description or a place or human name etc.\n   Question Category     How did serfdom develop in and then leave Russia ? DESCRIPTION   What films featured the character Popeye Doyle ? ENTITY   How can I find a list of celebrities ' real names ? DESCRIPTION   What fowl grabs the spotlight after the Chinese Year of the Monkey ? ENTITY   What is the full form of .com ? ABBREVIATION   What contemptible scoundrel stole the cork from my lunch ? HUMAN   What team did baseball \u0026rsquo;s St. Louis Browns become ? HUMAN    Category can take following values - HUMAN, ENTITY, DESCRIPTION, NUMERIC, LOCATION.\n   Solid #   A solid is a unit of computation in a data pipeline. Typically, you\u0026rsquo;ll define solids by annotating ordinary Python functions with the @solid decorator.\n Our first solid get_training_dataset loads the data from csv and returns\n texts (List[str]) - Question target (List[str]) - Category  1 2 3 4 5 6 7 8 9 10 11 12  @solid( output_defs=[ OutputDefinition(name=\u0026#34;texts\u0026#34;, is_required=True), OutputDefinition(name=\u0026#34;target\u0026#34;, is_required=True), ] ) def get_training_dataset(context): texts, target = dataloaders.get_input_dataset(INPUT_DATASET_LOC) context.log.info(f\u0026#34;Loaded data; N={len(texts)}, Targets={set(target)}\u0026#34;) yield Output(texts, \u0026#34;texts\u0026#34;) yield Output(target, \u0026#34;target\u0026#34;)      OutputDefinition #   To define multiple outputs, or to use a different output name than \u0026ldquo;result\u0026rdquo;, you can provide OutputDefinitions to the @solid decorator.\nWhen you have more than one output, you must yield an instance of the Output class to disambiguate between outputs.\n Let\u0026rsquo;s look at another solid preprocess_text.\n1 2 3 4 5  @solid def preprocess_text(context, texts): texts = text_preprocessing.preprocess_text(texts) context.log.info(f\u0026#34;Text pre-processing done; N={len(texts)}\u0026#34;) return texts   Here, we are doing some text preprocessing. Since we are not returning mulitple outputs here, we can avoid OutputDefinition.\n   Pipeline #   To execute our solid, we\u0026rsquo;ll embed it in an equally simple pipeline. A pipeline is a set of solids arranged into a DAG of computation. You\u0026rsquo;ll typically define pipelines by annotating ordinary Python functions with the @pipeline decorator.\n Let\u0026rsquo;s look at our Pipeline.\n1 2 3 4 5 6 7 8 9 10 11 12 13  @pipeline def ml_pipeline(): # 1. fetch training data texts, target = get_training_dataset() # 2. minimal text preprocessing # 3. tfidf vectorization vectorizer, X = get_vectorizer_and_features(preprocess_text(texts)) # 4. target encoding target_encoder, encoded_target = get_targetencoder_and_encoded_targets(target) # 5. train test split X_train, X_test, y_train, y_test = train_test_split(X, encoded_target) # 6. model training, validation, registry, artifact storage train_clf(X_train, X_test, y_train, y_test)   Here we call few solids like get_training_dataset, preprocess_text, get_vectorizer_and_features, etc.\nThese calls don\u0026rsquo;t actually execute the solids. Within the bodies of functions decorated with @pipeline, we use function calls to indicate the dependency structure of the solids making up the pipeline.\n   Executing Our First Pipeline #  Dagit to visualize our pipeline in Dagit, from the directory in which we have saved the pipeline file.\n1 2  $ dagit -f mlops/pipeline.py # Loading repository... Serving on http://127.0.0.1:3000   We can head over to the browser and look at the pipeline.\nTo execute the pipeline from the UI, head over to the playground section and click Launch Execution (at the bottom right).\nIf the pipeline executed successfully, you should see logs like this.\nThe pipeline we just executed registered our trained model using MLflow and stored the model artifacts using minio.\nWe will look at those components in the next part.\n","date":"Oct 26","permalink":"https://iutsav.dev/posts/mlops_template_2_dagster/","tags":["dagster"],"title":"MLOps Template, Part 2 - Dagster Pipeline"},{"categories":null,"contents":"In this series of posts, we will take a small dataset and go through various steps like building Data pipelines, ML workflow management, API development and Monitoring.\nThese steps are necessary for operationalization of any machine-learning based model.\n These posts are in no way exhaustive in covering the breadth of MLOps.Several key pieces like the CI/CD pipeline, monitoring for drift, etc are missing at the moment, which might get added later.\n    Stack  We will be using the following tools in this project\n Data Pipeline: Dagster ML registry: MLflow API Development: FastAPI Monitoring: ElasticAPM     Dev Setup     Poetry #  1 2 3  $ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - $ poetry --version # Poetry version 1.1.10      pre-commit #  1 2 3  $ pip install pre-commit $ pre-commit --version # pre-commit 2.15.0      Minio  Follow the instructions here - Minio installation.\nFor Mac users\n1  brew install minio/stable/minio      Install python packages  1 2 3 4 5  $ poetry install # Installing dependencies from lock file # No dependencies to install or update # Installing the current project: mlops (0.1.0)      MLflow  1 2 3 4 5 6 7 8 9 10  $ poetry shell $ export MLFLOW_S3_ENDPOINT_URL=http://127.0.0.1:9000 $ export AWS_ACCESS_KEY_ID=minioadmin $ export AWS_SECRET_ACCESS_KEY=minioadmin # make sure that the backend store and artifact locations are same in the .env file as well $ mlflow server \\  --backend-store-uri sqlite:///mlflow.db \\  --default-artifact-root s3://mlflow \\  --host 0.0.0.0      MinIO  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ export MINIO_ROOT_USER=minioadmin $ export MINIO_ROOT_PASSWORD=minioadmin $ mkdir minio_data $ minio server minio_data --console-address \u0026#34;:9001\u0026#34; # API: http://192.168.29.103:9000 http://10.119.80.13:9000 http://127.0.0.1:9000 # RootUser: minioadmin # RootPass: minioadmin # Console: http://192.168.29.103:9001 http://10.119.80.13:9001 http://127.0.0.1:9001 # RootUser: minioadmin # RootPass: minioadmin # Command-line: https://docs.min.io/docs/minio-client-quickstart-guide # $ mc alias set myminio http://192.168.29.103:9000 minioadmin minioadmin # Documentation: https://docs.min.io   Go to http://127.0.0.1:9001/buckets/ and create a bucket called mlflow.\n   Dagster  1 2  $ poetry shell $ dagit -f mlops/pipeline.py      ElasticAPM  1  $ docker-compose -f docker-compose-monitoring.yaml up      FastAPI  1 2 3  $ poetry shell $ export PYTHONPATH=. $ python mlops/app/application.py   ","date":"Oct 22","permalink":"https://iutsav.dev/posts/mlops_template_1_setup/","tags":["mlflow","minio","fastapi","elasticapm","dagster"],"title":"MLOps Template, Part 1 - Setup"},{"categories":null,"contents":"In this post, we will train a Word2Vec skip-gram model from scratch on some text and inspect the trained embeddings at the end.\nThe first step for using any kind of NLP pipeline is to vectorize the text. Traditionally, we used to do this using one-hot representation of vectors. This had various downsides like:\n Vectors tend to be very long as their size depends on the vocabulary size of the corpus(which grows with the corpus size). They don\u0026rsquo;t have any understanding of the text. They are sparse and can\u0026rsquo;t be used for any comparison as any two one-hot encoded vector from a set will be orthogonal.  Word2vec is able to tackle all these challenges. The architecture we define later will enable us to learn a distributed representation for each word/token in our corpus. The key idea being, we can represent a word by adding contextual information to it. Context refers to the words/tokens neighbouring the word/token of interest to us.\nOf course this technique is not perfect and has it\u0026rsquo;s own downsides. The key downside being we loose the word/token ordering while training the word2vec model, hence we are essentially dealing with a bag of words model. This makes the resulting embeddings not suitable for sentence level representations. We will cover other techniques later (RNNs, Transformers) that produce embeddings more suitable for sentences. For now our sole focus will be learning good word/token level embeddings.\n   First we will import the necessary libraries  1 2 3 4 5 6 7 8 9 10  from collections import Counter import re from typing import List import numpy as np import requests from sklearn.metrics.pairwise import cosine_similarity import torch import torch.nn as nn from torch.utils.data import DataLoader, Dataset   The whole process of training the embeddings can be broken down into the following key steps:\n We define a class TextProcessor that has methods to pre-process the text. fetch_text() fetches the texts directly from urls containing our text data. process_text() applies basic pre-procesing steps to the text and creates tokens using white space tokenization. prepare_vocab() creates token to index mapping and vice versa. Create torch Dataset and Dataloader where each index in the dataset will be a tuple, (center_word_ix, context_word_ix); i.e. pairs of indices of center word and context based on a pre defined window size.  The window side determines the number of context words around a center word. For example, window size of 5 will create the follwing center and context pairs for the sentence \u0026ldquo;a quick brown fox jumps\u0026rdquo; with \u0026ldquo;brown\u0026rdquo; being the center word - [(\u0026quot;brown\u0026quot;, \u0026quot;a\u0026quot;), (\u0026quot;brown\u0026quot;, \u0026quot;quick\u0026quot;), (\u0026quot;brown\u0026quot;, \u0026quot;fox\u0026quot;), (\u0026quot;brown\u0026quot;, \u0026quot;jumps\u0026quot;)]   Create our SkipGramModel model by defining a custom model on top of the torch.nn.Module. Define our loss function, optimizer and scheduler. Train the model for a certain number of epochs.  Finally, we will create a similarity matrix(cosine similarity) using the trained embeddings and explore similar embeddings for some word query.\n   Fetching and preparing the data  1 2 3 4 5 6 7 8 9 10  # We will be using the first four Harry Potter books. BASE_URL = \u0026#34;https://raw.githubusercontent.com/formcept/\u0026#34; BASE_URL += \u0026#34;whiteboard/master/nbviewer/notebooks/data/harrypotter\u0026#34; BOOK_URLS = [ f\u0026#34;{BASE_URL}/Book%201%20-%20The%20Philosopher\u0026#39;s%20Stone.txt\u0026#34;, f\u0026#34;{BASE_URL}/Book%202%20-%20The%20Chamber%20of%20Secrets.txt\u0026#34;, f\u0026#34;{BASE_URL}/Book%203%20-%20The%20Prisoner%20of%20Azkaban.txt\u0026#34;, f\u0026#34;{BASE_URL}/Book%204%20-%20The%20Goblet%20of%20Fire.txt\u0026#34; ]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  class TextProcessor: def __init__(self, urls: List[str]): self.urls = urls def fetch_text(self): self.text = \u0026#34;\u0026#34; for i, url in enumerate(self.urls): r = requests.get(url) self.text += \u0026#34;\\n\\n\\n\u0026#34; + r.text print(f\u0026#34;Fetched {i+1}/{len(self.urls)}urls ...\u0026#34;) def process_text(self): print(\u0026#34;Processing text ...\u0026#34;) sentences = re.split(\u0026#34;\\n{2,}\u0026#34;, self.text) print(f\u0026#34;{len(sentences)}sentences\u0026#34;) self.clean_sentences = [] for txt in sentences: txt = txt.strip().lower() txt = txt.replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) txt = \u0026#34; \u0026#34;.join(re.findall(\u0026#34;\\w+\u0026#34;, txt)) if txt: self.clean_sentences.append(txt) # Tokens and token counter will be used later to create a vocabulary for # us which will be used to map words/tokens to indices and vice versa, to # create training data and recreate the texts from predictions respectively. print(f\u0026#34;{len(sentences)}filtered sentences\u0026#34;) tokens = \u0026#34; \u0026#34;.join(self.clean_sentences).split(\u0026#34; \u0026#34;) print(f\u0026#34;{len(tokens)}tokens\u0026#34;) self.token_counter = Counter(tokens) print(f\u0026#34;{len(self.token_counter)}unique tokens\u0026#34;) def prepare_vocab(self, min_count: int): # min_count is the minimum number of times a token should appear in the # text to be considered in the vocabulary else they are assigned to a # default index which is equal to `len(vocab)`. self.w2ix = {} for i, (token, count) in enumerate( self.token_counter.most_common(len(self.token_counter)) ): if count \u0026lt; min_count: break else: self.w2ix[token] = i self.ix2w = {ix: w for w, ix in self.w2ix.items()} # Assign default index to rest of the tokens n = len(self.w2ix) for token, _ in self.token_counter.most_common(len(self.token_counter)): if token not in self.w2ix: self.w2ix[token] = n self.ix2w[n] = \u0026#34;\u0026lt;unk\u0026gt;\u0026#34; self.vocab_size = n + 1 print(f\u0026#34;Vocab size: {self.vocab_size}\u0026#34;)   1 2 3 4  text_processor = TextProcessor(BOOK_URLS) text_processor.fetch_text() text_processor.process_text() text_processor.prepare_vocab(min_count=20)   Fetched 1/4 urls ... Fetched 2/4 urls ... Fetched 3/4 urls ... Fetched 4/4 urls ... Processing text ... 20033 sentences 20033 filtered sentences 501854 tokens 15078 unique tokens Vocab size: 2103     Preparing the torch Dataset and Dataloader  The Dataloader will enable us to send data in batches during the forward pass of the model training.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  class HarryPotterDataset(Dataset): def __init__(self, text_processor: TextProcessor, window_size: int=3): self.data = [] self.text_processor = text_processor self._init_data() def _init_data(self): for txt in text_processor.clean_sentences: splits = txt.split(\u0026#34; \u0026#34;) for i in range(window_size, len(splits) -1): center_word = splits[i] window_words = splits[i-window_size:i]+splits[i+1:i+window_size+1] for context in window_words: # Each data point under self.data will be a tuple with index 0 # containing the index(w2ix) for center_word and # index 1 containing the index(w2ix) for context_word self.data.append( (text_processor.w2ix[center_word], text_processor.w2ix[context]) ) def __getitem__(self, ix: int): return self.data[ix] def __len__(self): return len(self.data)   1 2 3  dataset = HarryPotterDataset(text_processor, 3) dataloader = DataLoader(dataset, batch_size=64, shuffle=True) len(dataset)   2489461     Creating the Word2Vec model using torch.nn.Module  1 2 3 4 5 6 7 8  class SkipGramModel(nn.Module): def __init__(self, vocab_size: int, embedding_size: int=50): super().__init__() self.embedding = nn.Embedding(vocab_size, embedding_size) self.linear = nn.Linear(embedding_size, vocab_size) def forward(self, x): return self.linear(self.embedding(x))   The model architecure is Embedding -\u0026gt; Linear -\u0026gt; Softmax.\nEmbedding is nothing but \u0026ldquo;A simple lookup table that stores embeddings of a fixed dictionary and size\u0026rdquo; - pytorch documentation.\nBased on our model architecture, this layer will contain the trained token/word embeddings of interest to us and we will discard the weight matrix from the Linear layer.\nIn each forward pass we:\n Pass indices of center word as input x. These indices are looked up in the self.embedding table. We transform the vector from last step using the Linear layer to get another vector with dimension vocab_size. Finally we apply softmax and calcualte loss.  A trained model should output high values for the context indices for a given center word.\n1 2  device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = SkipGramModel(text_processor.vocab_size, embedding_size=30).to(device)      Defining our loss function, optimizer and scheduler  1 2 3  criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.005) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)   1 2  N_EPOCHS = 10 STEP_TO_LOG = 10000      Training the model  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  epoch_losses, batch_losses = [], [] for epoch in range(N_EPOCHS): losses = [] for i, data in enumerate(dataloader): inputs, outputs = data[0].long().to(device), data[1].long().to(device) pred = model(inputs) loss = criterion(pred, outputs) losses.append(loss.item()) loss.backward() optimizer.step() optimizer.zero_grad() if i % STEP_TO_LOG == 0: print(f\u0026#34;epoch {epoch + 1}; steps={i:\u0026gt;5}/{len(dataloader)}; loss={loss:\u0026lt;.5f}\u0026#34;) scheduler.step() batch_losses += losses epoch_losses.append(np.mean(losses)) print(f\u0026#34;epoch {epoch + 1}; lr={scheduler._last_lr[0]}; loss={epoch_losses[-1]:\u0026lt;.5f}\\n\u0026#34;)   epoch 1; steps= 0/38898; loss=7.80020 epoch 1; steps=10000/38898; loss=5.55375 epoch 1; steps=20000/38898; loss=5.58139 epoch 1; steps=30000/38898; loss=5.27452 epoch 1 ; lr=0.005; loss=5.60440 epoch 2; steps= 0/38898; loss=4.85012 epoch 2; steps=10000/38898; loss=5.81531 epoch 2; steps=20000/38898; loss=5.07261 epoch 2; steps=30000/38898; loss=5.62864 epoch 2 ; lr=0.005; loss=5.52841 epoch 3; steps= 0/38898; loss=5.27015 epoch 3; steps=10000/38898; loss=5.55336 epoch 3; steps=20000/38898; loss=5.67527 epoch 3; steps=30000/38898; loss=5.60372 epoch 3 ; lr=0.005; loss=5.52233 epoch 4; steps= 0/38898; loss=5.50681 epoch 4; steps=10000/38898; loss=5.48357 epoch 4; steps=20000/38898; loss=5.18897 epoch 4; steps=30000/38898; loss=5.74382 epoch 4 ; lr=0.005; loss=5.52111 epoch 5; steps= 0/38898; loss=5.67365 epoch 5; steps=10000/38898; loss=5.66463 epoch 5; steps=20000/38898; loss=5.82155 epoch 5; steps=30000/38898; loss=5.77744 epoch 5 ; lr=0.0025; loss=5.52081 epoch 6; steps= 0/38898; loss=5.54513 epoch 6; steps=10000/38898; loss=5.40478 epoch 6; steps=20000/38898; loss=5.71839 epoch 6; steps=30000/38898; loss=5.23344 epoch 6 ; lr=0.0025; loss=5.47719 epoch 7; steps= 0/38898; loss=4.93150 epoch 7; steps=10000/38898; loss=5.09374 epoch 7; steps=20000/38898; loss=5.47905 epoch 7; steps=30000/38898; loss=5.92455 epoch 7 ; lr=0.0025; loss=5.47128 epoch 8; steps= 0/38898; loss=5.22414 epoch 8; steps=10000/38898; loss=5.67435 epoch 8; steps=20000/38898; loss=5.55853 epoch 8; steps=30000/38898; loss=5.28791 epoch 8 ; lr=0.0025; loss=5.47099 epoch 9; steps= 0/38898; loss=5.33284 epoch 9; steps=10000/38898; loss=5.80998 epoch 9; steps=20000/38898; loss=6.04806 epoch 9; steps=30000/38898; loss=5.58979 epoch 9 ; lr=0.0025; loss=5.47106 epoch 10; steps= 0/38898; loss=5.09184 epoch 10; steps=10000/38898; loss=5.18391 epoch 10; steps=20000/38898; loss=5.66316 epoch 10; steps=30000/38898; loss=5.44824 epoch 10 ; lr=0.00125; loss=5.47111  1 2  weights = model.embedding.weight.detach().cpu().numpy() weights.shape   (2103, 30)     Creating the similarity matrix  1  similarity = cosine_similarity(weights, weights)   1 2 3 4 5 6 7 8 9 10 11  def get_similar_words(input_word: str, n: int): if input_word not in text_processor.w2ix: print(\u0026#34;word not in vocab\u0026#34;) else: input_word_ix = text_processor.w2ix[input_word] similarity_vector = similarity[input_word_ix] most_similar_ixs = np.argsort(similarity_vector)[-n:][::-1] most_similar_words = [(text_processor.ix2w[ix], similarity_vector[ix]) for ix in most_similar_ixs] for word, score in most_similar_words: print(f\u0026#34; \u0026gt; {word}, score={score:.2f}\u0026#34;)      Exploring similar embeddings for some queries  1  get_similar_words(\u0026#34;snape\u0026#34;, 10)    \u0026gt; snape, score= 1.00 \u0026gt; karkaroff, score= 0.81 \u0026gt; dumbledore, score= 0.79 \u0026gt; lupin, score= 0.77 \u0026gt; mcgonagall, score= 0.76 \u0026gt; moody, score= 0.74 \u0026gt; professor, score= 0.72 \u0026gt; flitwick, score= 0.71 \u0026gt; trelawney, score= 0.69 \u0026gt; quirrell, score= 0.69  1  get_similar_words(\u0026#34;vernon\u0026#34;, 10)    \u0026gt; vernon, score= 1.00 \u0026gt; aunt, score= 0.81 \u0026gt; uncle, score= 0.79 \u0026gt; dudley, score= 0.78 \u0026gt; petunia, score= 0.70 \u0026gt; marge, score= 0.62 \u0026gt; furious, score= 0.59 \u0026gt; telephone, score= 0.53 \u0026gt; sister, score= 0.53 \u0026gt; company, score= 0.52  1  get_similar_words(\u0026#34;muggle\u0026#34;, 10)    \u0026gt; muggle, score= 1.00 \u0026gt; wizarding, score= 0.81 \u0026gt; old, score= 0.65 \u0026gt; yer, score= 0.63 \u0026gt; international, score= 0.61 \u0026gt; important, score= 0.61 \u0026gt; our, score= 0.61 \u0026gt; is, score= 0.61 \u0026gt; young, score= 0.60 \u0026gt; yourself, score= 0.60  1  get_similar_words(\u0026#34;harry\u0026#34;, 10)    \u0026gt; harry, score= 1.00 \u0026gt; lupin, score= 0.64 \u0026gt; ron, score= 0.63 \u0026gt; hermione, score= 0.61 \u0026gt; she, score= 0.57 \u0026gt; he, score= 0.57 \u0026gt; colin, score= 0.54 \u0026gt; dumbledore, score= 0.52 \u0026gt; furiously, score= 0.51 \u0026gt; moody, score= 0.49  1  get_similar_words(\u0026#34;slytherin\u0026#34;, 10)    \u0026gt; slytherin, score= 1.00 \u0026gt; gryffindor, score= 0.79 \u0026gt; hufflepuff, score= 0.75 \u0026gt; team, score= 0.71 \u0026gt; heir, score= 0.71 \u0026gt; ravenclaw, score= 0.66 \u0026gt; goal, score= 0.64 \u0026gt; seeker, score= 0.64 \u0026gt; wood, score= 0.60 \u0026gt; irish, score= 0.60  1  get_similar_words(\u0026#34;malfoy\u0026#34;, 10)    \u0026gt; malfoy, score= 1.00 \u0026gt; goyle, score= 0.77 \u0026gt; crabbe, score= 0.64 \u0026gt; draco, score= 0.61 \u0026gt; diggory, score= 0.59 \u0026gt; lucius, score= 0.57 \u0026gt; weasley, score= 0.51 \u0026gt; laughing, score= 0.50 \u0026gt; pettigrew, score= 0.50 \u0026gt; percy, score= 0.50  There are various Intrisic and Extrinsic evaluation criterias for these embeddings. For Evaluation and Interpretation of the embeddings take a look here.\n","date":"Mar 12","permalink":"https://iutsav.dev/posts/word2vec_skipgram/","tags":["python","word2vec","skipgram"],"title":"Revisiting Word2Vec skip-gram model"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://iutsav.dev/archives/","tags":null,"title":"Archive"}]
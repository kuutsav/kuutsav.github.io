[{"categories":null,"contents":"","date":"Jan 26","permalink":"https://iutsav.dev/projects/b_information_retrieval/","tags":null,"title":"information-retrieval"},{"categories":null,"contents":"","date":"Jan 26","permalink":"https://iutsav.dev/projects/a_leetcomp/","tags":null,"title":"LeetComp"},{"categories":null,"contents":"IR in it\u0026rsquo;s most basic form answers the question \u0026ldquo;how relevant is a given query for a document\u0026rdquo;. The challenge is that we don\u0026rsquo;t have just 1 document but potentially millions or billions of documents. So the key challenge is - how can we efficiently find this \u0026ldquo;needle in the haystack\u0026rdquo; or the \u0026ldquo;relevant documents for a query\u0026rdquo;.\nHere, document refers to any kind of text document, typically these could be web pages, emails, plain text documents, etc. There are many technicalities to consider in real world applicatations of IR like cleaning up the markup in case of web pages. The documents could also be in different languagues, encodings, etc. These variations present their own unique sets of challenges. For example, it\u0026rsquo;s common to build separate pre-processing text piplines for each languague.\nRelevance To answer the question \u0026ldquo;how relevant is a given query for a document\u0026rdquo;, we first need to precisely define relevance.\nRelevance in the context of queries and documents captures the following key ideas:\nA query can consist of many words, we break the query down into these individual words using a text processing pipeline which we will cover later. If a word appears more often in a document, it\u0026rsquo;s more relevant. We can capture this by counting the words in a document . If a document is longer, words will tend to appear more often. So we need to take the document length into account as well. It would be pretty slow if we start counting the words in all the documents after a query arrives, hence we need to store the counts in a data structure that supports efficient lookups for a query. Fig. 1. Illustration of IR with a query and documents sorted by relevancy. Inverted Index An inverted index helps us solve the last point we mentioned under Relevance, \u0026ldquo;It would be pretty slow if we start counting the words in all the documents after a query arrives, hence we need to store the counts in a data structure that supports efficient lookups for a query.\u0026rdquo;\nInverted index allows us to efficienty retrieve documents from large collections. It does this by storing term/word statistics from the documents beforehand(that the scoring model needs).\nThe statistics stored in the inverted index are:\nDocument frequency: How many documents contain the term. Term frequency per document: How often does the term appear in a document. Document length Average document length The statistics are saved in a format that is accessible by a given term/word.\nFig. 2. Illustration of statistics stored in an Inverted Index. The term frequencies are stored in a \u0026ldquo;posting list\u0026rdquo; which is nothing but a list of document id and term frequency pairs. Every document gets an internal document id; which can be a simple integers for non web-scale data.\nCreating the Inverted Index The process of creating the inverted index involves creating a text processing pipeline which is applied both the documents as well as incoming queries.\nA typical pipeline looks like: Tokenization -\u0026gt; Stemming -\u0026gt; Stop words removal.\nTokenization Tokenization essentially involves splitting the query or document into invdividual terms/words. A naive baseline could split each query/document by whitespace and punctuation character. Improvements over the naive model typically involes keeping the abbreviations, names, numbers together as one token, etc. For example, look at the stanford tokenizer.\n$ cat \u0026gt;sample.txt \u0026#34;Oh, no,\u0026#34; she\u0026#39;s saying, \u0026#34;our $400 blender can\u0026#39;t handle something this hard!\u0026#34; $ java edu.stanford.nlp.process.PTBTokenizer sample.txt `` Oh , no , \u0026#39;\u0026#39; she \u0026#39;s saying , `` our $ 400 blender ca n\u0026#39;t handle something this hard ! \u0026#39;\u0026#39; PTBTokenizer tokenized 23 tokens at 370.97 tokens per second. Stemming Stemming involves reducing the terms/words to their \u0026ldquo;roots\u0026rdquo; before indexing. An advanved form of stemming called Lemmatization invloves reducing the inflectional/variant forms to base form (am, are, is -\u0026gt; be). Lemmatization is computationally expensive.\nFor grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance: am, are, is -\u0026gt; be, car, cars, car's, cars' -\u0026gt; car. The result of this mapping of text will be something like: the boy's cars are different colors -\u0026gt; the boy car be differ color\nHowever, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\nThe most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is Porter\u0026rsquo;s algorithm (Porter, 1980).\nStemming and lemmatization\nStop word removal This simply involves removing the most common words that appear across all the documents. Typically, articles and pronouns are generally classified as stop words. Stop words play no significance in the scoring algorithms used for classic IR hence we remove them.\nFor example, \u0026ldquo;a\u0026rdquo;, \u0026ldquo;the\u0026rdquo;, \u0026ldquo;there\u0026rdquo;, etc.\nSearch and Relevance Scoring Search workflow The search workflow typically invloves:\nPassing the query throught the pre processing pipeline we just discussed to get the terms. Look up the statistics for each term from the inverted index. Use a scoring model that uses the term statistics to score the documents. Sort the documents by the scores in a descending order and show to the user. Fig. 3. Illustration of the search workflow using Inverted Index. Note that a document could be relevant without containing the exact query terms. This is the biggest drawback of the classic IR techniques we are discussing here.\nDense retrieval techniques which we will cover later help us address this problem.\nTypes of queries Once the query has been broken into individual words, there are many ways they can be used to fetch statistics from the inverted index:\nExact matching: Match full words or concatenate multiple query words with \u0026ldquo;or\u0026rdquo;. Boolean queries: \u0026ldquo;and\u0026rdquo; / \u0026ldquo;or\u0026rdquo; / \u0026ldquo;not\u0026rdquo; operators between words. Expanded queries: Ddd synonyms and other similar words into the query. Beyond this we can also augment the queries using wildcard querie, phonetic queries, etc.\nSpell checking Another way to improve the quality of retrieval is to use spell correctors to:\nCorrect the docuemnts being indexed. Correct user queries to retrieve correct documents - e.g. the google sytle os \u0026ldquo;did you mean\u0026rdquo;. Scoring models The scoring model captures relevance in a mathematical model. Note that here we are only cosidering the free text queries here or \u0026ldquo;ad-hoc\u0026rdquo; document retrieval, other factors could also come into play in real life scenarios like pagerank, recency, click counts etc.\nAnother drawback of the scoring models we are about the discuss is the fact they don\u0026rsquo;t consider meaning of the terms. Terms/words are essentially just discrete symbols. Similarly documents are stream of meaningless symbols. The word order is not important as well.\nDense retrieval techniques which we will cover later help us address this problem.\nTF_IDF The TF_IDF has two components:\nTerm Frequency tf(t,d): How often does the term t appear in the document d. Using the raw frequencies here is not the best solution for IR problems. We either use relative frequenies or log to dampen the frequencies tf(t,d) = log(1 + tf(t,d)). Inverse Document Frequency idf(t): It\u0026rsquo;s a measure of the \u0026ldquo;informativeness\u0026rdquo; of a term for a document. Not that rare terms are more informative than frequent terms. Common way of defining IDF is idf(t) = log(|D| / df(t)). Here |D| is the total number of documents and df(t) is the number of documents in which the term t appears. where ‚àë is the sum over all the terms in the query, ùë°ùëì(ùë°,ùëë) is the frequency of term t in document, |ùê∑| is the total number of documents, ùëëùëì(ùë°) is the number of documents in which the term t appears.\nBeyond IR, TF_IDF is also used for other NLP tasks like finding keywords in a document, Latent Sematic Analysis(LSA), classification, etc.\nAlthough the formulation in scikit-learn is not exactly the same, we can still look at TF_IDF in action.\n1 2 3 4 5 6 7 8 9 10 11 12 docs = [\u0026#34;the quick brown fox jumps over the lazy dog\u0026#34;, \u0026#34;the fox and the crow\u0026#34;, \u0026#34;the smart crow\u0026#34;] import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(docs) df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names_out()) df.index = docs df For downstream ML tasks like classification(given we have the class labels), this matrix notation works perfectly. But computing, storing and updating this in matrix form for retrieval problems is not feasible due to the sparsity of this matrix for large number of documents.\nFor example, if we have 1m documents with average term length of 1000 and vocabulary size of 100k(pretty reasonable at scale), we end up with a matrix of size 1,000,000 x 100,000 of which ~1,000,000 x 90,000 or ~90% of the terms will be 0. Though this is a very conservative estimate, it makes it obvious that storing the TF_IDF for web scale data is not feasible in this format. We use inverted index we discussed earlier to store this info.\nBM25 aka \u0026ldquo;BestMatch25\u0026rdquo; BM25 is the backbone of most open source text based search engines over the last decade. It improves over the TF_IDF by taking into account the normalization of document length by average document length.\nThere are two hyperparameters k and b that can be tuned to define the influence of document length normalization.\nwhere ‚àë is the sum over all the terms in the query, ùë°ùëì(ùë°,ùëë) is the frequency of term t in document d, ùëëùëô(ùëë) is the document length, ùëéùë£ùëîùëëùëô is the average document length, |ùê∑| is the Total number of documents, ùëëùëì(ùë°) is the number of documents in which the term t appears, ùëò1,b are the hyperparameters.\nNot that this formulation is simpler than the original formula. More complex parts related to query length have not much practical implications for IR. The key difference between TF_IDF vs BM25 is that the term frequency saturation is stronger in BM25.\nThe hyperparametrs:\nk: Controls term frequency scaling. b: Conrols the document length normalization. Typical values of k and b are 1.2 \u0026lt; k \u0026lt; 2, 0.5 \u0026lt; b \u0026lt; 0.8.\nThere are other forumations of BM25 that take into account the title, abstract, headers etc. from the documents. We can essentially control the weights we want to assign to each component.\nOversimiplified implementation of the Inverted Index (TF_IDF) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 from collections import Counter, defaultdict import math import re from typing import List, Tuple import spacy from spacy.lang.en.stop_words import STOP_WORDS nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) class InvertedIndex: \u0026#34;\u0026#34;\u0026#34; A toy implementation of the \u0026#34;Inverted Index\u0026#34;. \u0026#34;\u0026#34;\u0026#34; def __init__(self): # note that we are not using a postings list here but # a dictionary to maintain the term frequency counts self._index = defaultdict(lambda: defaultdict(int)) self._doc_lengths = [] def _check_txt_not_empty(self, txt: str) -\u0026gt; None: if not txt: raise Exception(\u0026#34;txt can\u0026#39;t be empty\u0026#34;) def get_terms_from_text(self, txt: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34; Pre-processing pipeline applied to both the query and the documents. Args: txt (str): Input query or document. Returns: List[str]: Terms from a query/document. \u0026#34;\u0026#34;\u0026#34; self._check_txt_not_empty(txt) doc = nlp(\u0026#34; \u0026#34;.join(re.findall(\u0026#34;\\w+\u0026#34;, txt.lower()))) return [token.lemma_.lower() for token in doc if token.lemma_.lower() not in STOP_WORDS] def _update_doc_lenghts(self, doc_length: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Utility method that helps maintain the statistic \u0026#34;Document lenghts\u0026#34;. Args: doc_length (int): Length of a new document. \u0026#34;\u0026#34;\u0026#34; self._doc_lengths.append(doc_length) def _add_term_to_index(self, term: str, doc_id: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Utility method that helps maintain the statistic \u0026#34;Term frequencies\u0026#34;. Args: term (str): Term from a document. doc_id (str): Document Id. \u0026#34;\u0026#34;\u0026#34; self._index[term][doc_id] += 1 def index_text(self, txt: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Method used to update the inverted index for a document. Args: txt (str): Input document. \u0026#34;\u0026#34;\u0026#34; self._check_txt_not_empty(txt) terms = self.get_terms_from_text(txt) self._update_doc_lenghts(len(terms)) for term in terms: self._add_term_to_index(term, len(self._doc_lengths)-1) def get_docs_from_term(self, term: str) -\u0026gt; defaultdict: \u0026#34;\u0026#34;\u0026#34; Returns document ids and term frequencies for a an input term. Args: term (str): Input term. Returns: defaultdict: Document ids and term frequencies. \u0026#34;\u0026#34;\u0026#34; return self._index[term] def score_query( self, txt: str, k: int=10, match_type: str=\u0026#34;or\u0026#34; ) -\u0026gt; List[Tuple[int, float]]: \u0026#34;\u0026#34;\u0026#34; Returns documents and their scores for a query. Documents are sorted in the descening order of their scores. Args: txt (str): Input query. k (int): N top documents to return. match_type (str): Boolean filter to either take an intersection(and) or union(and) when aggregating documents by terms. Returns: List[Tuple[int, float]]: List of document ids and scores. \u0026#34;\u0026#34;\u0026#34; assert match_type in (\u0026#34;or\u0026#34;, \u0026#34;and\u0026#34;), f\u0026#34;{match_type} is not a valid boolean filter\u0026#34; self._check_txt_not_empty(txt) terms = self.get_terms_from_text(txt) valid_docs = set() for i, term in enumerate(terms): if i == 0 or match_type == \u0026#34;or\u0026#34;: valid_docs.update(self.get_docs_from_term(term).keys()) elif match_type == \u0026#34;and\u0026#34;: valid_docs.intersection_update(self.get_docs_from_term(term).keys()) doc_scores = defaultdict(float) for term in terms: for doc, term_frequency in self._index[term].items(): if doc in valid_docs: score = math.log(1+term_frequency) * \\ math.log(len(self._doc_lengths)/len(self._index[term])) doc_scores[doc] += score return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:k] def __len__(self): return len(self._doc_lengths) 1 2 3 4 5 6 7 8 9 10 from sklearn.datasets import fetch_20newsgroups inverted_index = InvertedIndex() documents = fetch_20newsgroups()[\u0026#34;data\u0026#34;] print(\u0026#34;Indexing ...\u0026#34;) for i, doc in enumerate(documents[:1000]): # Index the first 1000 documents if i \u0026gt; 0 and i % 100 == 0: print(i, end=\u0026#34; \u0026#34;) inverted_index.index_text(doc) Indexing ... 100 200 300 400 500 600 700 800 900 1 query = \u0026#34;Computer ram\u0026#34; 1 2 for doc_id, score in inverted_index.score_query(query, k=5, match_type=\u0026#34;or\u0026#34;): print(f\u0026#34;Score : {score:4.3f}\\nDocument : {repr(documents[doc_id])[:500]} ...\\n---\\n\u0026#34;) Score : 8.245\nDocument : \u0026lsquo;From: richg@sequent.com (Richard Garrett)\\nSubject: Computers for sale ( PC and amiga )\\nArticle-I.D.: sequent.1993Apr21.151726.26547\\nDistribution: na\\nOrganization: Sequent Computer Systems, Inc.\\nLines: 57\\nNntp-Posting-Host: crg8.sequent.com\\n\\nIts time for a little house cleaning after my PC upgrade. I have the following\\nfor sale:\\n\\nLeading Technology PC partner (286) sytsem. includes\\n\\t80286 12mhz intel cpu\\n\\t85Mb IDE drive (brand new - canabalized from new system)\\n\\t3.5 and 5.24 f \u0026hellip;\nScore : 7.497\nDocument : \u0026ldquo;From: rosa@ghost.dsi.unimi.it (massimo rossi)\\nSubject: ide \u0026amp;scsi controller\\nOrganization: Computer Science Dep. - Milan University\\nLines: 16\\n\\nhi folks\\ni have 2 hd first is an seagate 130mb\\nthe second a cdc 340mb (with a future domain no ram)\\ni\u0026rsquo;d like to change my 2 controller ide \u0026amp; scsi and buy\\na new one with ram (at least 1mb) that could controll \\nall of them\\nany companies?\\nhow many $?\\nand is it possible via hw or via sw select how divide\\nthe ram cache for 2 hd? (for example usin \u0026hellip;\nScore : 6.218\nDocument : \u0026ldquo;From: lingeke2@mentor.cc.purdue.edu (Ken Linger)\\nSubject: 32 Bit System Zone\\nOrganization: Purdue University\\nX-Newsreader: TIN [version 1.1 PL8]\\nLines: 32\\n\\nA week or so ago, I posted about a problem with my SE/30: I have 20 megs\\nor true RAM, yet if I set my extensions to use a large amount of memory\\n(total of all extensions) then my system will crash before the finder\\ncomes up. What I meant was having a large amount of fonts load, or\\nsounds, or huge disk caches with a control panel \u0026hellip;\nScore : 5.523\nDocument : \u0026ldquo;From: ebosco@us.oracle.com (Eric Bosco)\\nSubject: Windows 3.1 keeps crashing: Please HELP\\nNntp-Posting-Host: monica.us.oracle.com\\nReply-To: ebosco@us.oracle.com\\nOrganization: Oracle Corp., Redwood Shores CA\\nX-Disclaimer: This message was written by an unauthenticated user\\n at Oracle Corporation. The opinions expressed are those\\n of the user and not necessarily those of Oracle.\\nLines: 41\\n\\n\\nAs the subjects says, Windows 3.1 keeps crashing (givinh me GPF) on me \u0026hellip;\nScore : 5.356\nDocument : \u0026lsquo;From: afung@athena.mit.edu (Archon Fung)\\nSubject: wrong RAM in Duo?\\nOrganization: Massachusetts Institute of Technology\\nLines: 9\\nDistribution: world\\nNNTP-Posting-Host: thobbes.mit.edu\\n\\nA few posts back, somebody mentioned that the Duo might crash if it has\\nthe wrong kind (non-self refreshing) of RAM in it. My Duo crashes\\nsometimes after sleep, and I am wondering if there is any software which\\nwill tell me whether or not I have the right kind of RAM installed. I\\nhad thought that the \u0026hellip;\n1 2 for doc_id, score in inverted_index.score_query(query, k=5, match_type=\u0026#34;and\u0026#34;): print(f\u0026#34;Score : {score:4.3f}\\nDocument : {repr(documents[doc_id])[:500]} ...\\n---\\n\u0026#34;) Score : 8.245\nDocument : \u0026lsquo;From: richg@sequent.com (Richard Garrett)\\nSubject: Computers for sale ( PC and amiga )\\nArticle-I.D.: sequent.1993Apr21.151726.26547\\nDistribution: na\\nOrganization: Sequent Computer Systems, Inc.\\nLines: 57\\nNntp-Posting-Host: crg8.sequent.com\\n\\nIts time for a little house cleaning after my PC upgrade. I have the following\\nfor sale:\\n\\nLeading Technology PC partner (286) sytsem. includes\\n\\t80286 12mhz intel cpu\\n\\t85Mb IDE drive (brand new - canabalized from new system)\\n\\t3.5 and 5.24 f \u0026hellip;\nScore : 7.497\nDocument : \u0026ldquo;From: rosa@ghost.dsi.unimi.it (massimo rossi)\\nSubject: ide \u0026amp;scsi controller\\nOrganization: Computer Science Dep. - Milan University\\nLines: 16\\n\\nhi folks\\ni have 2 hd first is an seagate 130mb\\nthe second a cdc 340mb (with a future domain no ram)\\ni\u0026rsquo;d like to change my 2 controller ide \u0026amp; scsi and buy\\na new one with ram (at least 1mb) that could controll \\nall of them\\nany companies?\\nhow many $?\\nand is it possible via hw or via sw select how divide\\nthe ram cache for 2 hd? (for example usin \u0026hellip;\nScore : 5.523\nDocument : \u0026ldquo;From: ebosco@us.oracle.com (Eric Bosco)\\nSubject: Windows 3.1 keeps crashing: Please HELP\\nNntp-Posting-Host: monica.us.oracle.com\\nReply-To: ebosco@us.oracle.com\\nOrganization: Oracle Corp., Redwood Shores CA\\nX-Disclaimer: This message was written by an unauthenticated user\\n at Oracle Corporation. The opinions expressed are those\\n of the user and not necessarily those of Oracle.\\nLines: 41\\n\\n\\nAs the subjects says, Windows 3.1 keeps crashing (givinh me GPF) on me \u0026hellip;\nScore : 5.236\nDocument : \u0026ldquo;Subject: XV under MS-DOS ?!?\\nFrom: NO E-MAIL ADDRESS@eicn.etna.ch\\nOrganization: EICN, Switzerland\\nLines: 24\\n\\nHi \u0026hellip; Recently I found XV for MS-DOS in a subdirectory of GNU-CC (GNUISH). I \\nuse frequently XV on a Sun Spark Station 1 and I never had problems, but when I\\nstart it on my computer with -h option, it display the help menu and when I\\nstart it with a GIF-File my Hard disk turns 2 or 3 seconds and the prompt come\\nback.\\n\\nMy computer is a little 386/25 with copro, 4 Mega rams, Ts \u0026hellip;\nScore : 4.705\nDocument : \u0026ldquo;From: Dale_Adams@gateway.qm.apple.com (Dale Adams)\\nSubject: Re: HELP INSTALL RAM ON CENTRIS 610\\nOrganization: Apple Computer Inc.\\nLines: 23\\n\\nIn article C5115s.5Fy@murdoch.acc.Virginia.EDU \\njht9e@faraday.clas.Virginia.EDU (Jason Harvey Titus) writes:\\n\u0026gt; I had asked everyone about problems installing a 4 meg\\n\u0026gt; simm and an 8 meg simm in my Centris 610, but the folks at the\\n\u0026gt; local Apple store called the folks in Cupertino and found that\\n\u0026gt; you can\u0026rsquo;t have simms of different speeds \u0026hellip;\nReferences [1] Sebastian Hofst√§tter. \u0026ldquo;Advanced Information Retrieval 2021 @ TU Wien\u0026rdquo;\n","date":"Aug 01","permalink":"https://iutsav.dev/posts/information_retrieval_1_classic_ir/","tags":["python","information-retrievel","inverted-index"],"title":"Information Retrieval, Part 1 - The Inverted Index"},{"categories":null,"contents":"Given a set of N objects that support the following two commands:\nUnion: Connect two objects. Find/Connected: Is there a path connecting the two obejcts? For example, consider this set of 10 objects\nAfter few union commands union(2, 3), union(6, 5), union(8, 6), union(10, 8) the state of the system changes to\nWe can query the above system to find if two objects are connected or not like find(0, 1) == False, find(1, 2) == True, find(4, 9) == True, find(8, 1) == False\nTo be formal, we can say that \u0026ldquo;connected to\u0026rdquo; has the following properties:\nReflexive: a is connected to a. Symmetric: if a is connected to b, then b is conntected to a. Transitive: if a is connected to b and b is conntected to c, then a is connected to c. Another common terminology in dynamic connectivity problems is Connected components. It refers to the maximal set of objects that are mutually connected. In the above example, the conntected components are {0), {1, 2}, {3}, {4, 5, 7, 9}, {6}, {8}. The union-find algorithms we are going to implement below can help us model objects of many different kinds. Some of the practical examples include:\nPixels in an image. Computers in a network. Friends in a network. Grid system for path finding problems. When we are programming the union-find operations, it\u0026rsquo;s convenient to represent the set of objects and their connectivity using a list from 0 to N-1. The overall goal is to design a data structure and algorithm for union-find that is effecient when:\nNumber of ojects N can be huge. Number of operations M can be huge. Find and Union queries can be intermixed. Quick-Find We will used id[] to store all the obecjts. The index will refer to the objects and the value will indicate the connectivity between two indices.\nFor example id[] = [0, 1, 1, 3, 5, 5, 6, 5, 8, 5]. Here, objects {1, 2} are connected and {4, 5, 7, 9} are connected.\nFind: Check if indices a and b have the same id. Union: To merge indices a and b, change all entries whose id equals id[a] to id[b]. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from collections import defaultdict, Counter import random import time import matplotlib.pyplot as plt import numpy as np def plot(sequences): plt.figure(figsize=(12, 6)) for k, seq, label in sequences: plt.plot(k, seq, label=label) plt.legend() return plt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class QuickFind: def __init__(self, n:int=10): self.id = list(range(n)) def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def find(self, a:int, b:int) -\u0026gt; bool: return self.id[a] == self.id[b] def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.id[a] root_of_b = self.id[b] for i, _ in enumerate(self.id): if self.id[i] == root_of_a: self.id[i] = root_of_b def simulate(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) if random.random() \u0026gt; 0.5: st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) else: st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st) It\u0026rsquo;s easy to observe the worst case time complexity of QuickFind:\nInitialize: O(N) Union: O(N) Find: O(1) Even though the operations don\u0026rsquo;t look too bad on their own, for a sequence of N union commands on N objects(a very common operation for such problems), this becomes O(N^2).\n1 2 3 4 5 6 7 8 9 timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 11000, 1000): print(i, end=\u0026#34; \u0026#34;) qf = QuickFind(i) st = time.time() qf.simulate() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st) simulating 100 1100 2100 3100 4100 5100 6100 7100 8100 9100 10100 1 2 3 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]); 1 2 3 4 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]); An improvement over QuickFind is the QuickUnion algorithm.\nQuick-Union Here also we use an array to store the objects, though the interpretation of the values in the array changes, nowid[i] is the parent of i. We are essentially using the array to create a tree like structure. To find the root of any object i, we recursively traverse it\u0026rsquo;s parent till the index is same as the value, i.e. Root of i is id[id[id[\u0026hellip;id[i]\u0026hellip;]]].\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class QuickUnion: def __init__(self, n:int=10): self.id = list(range(n)) def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def root(self, ix:int) -\u0026gt; int: while ix != self.id[ix]: ix = self.id[ix] return ix def find(self, a:int, b:int) -\u0026gt; bool: return self.root(a) == self.root(b) def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.root(a) root_of_b = self.root(b) self.id[root_of_a] = root_of_b def simulate(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) if random.random() \u0026gt; 0.5: st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) else: st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st) def simulate_worst_case(self) -\u0026gt; None: n = len(self.id) for i in range(n-1): st = time.time() self.union(i, i+1) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) for i in range(n): st = time.time() self.find(i, i) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st) Here we can observe that the Union and Find operations have a time complexity of O(logN) on average, given that the tree remains balanced. The problem occurs when the tree gets too tall. In the worst case, we can end up with one very tall skinny tree where the time complexity of Find operation becomes O(N). In this case, we again end up with O(N^2) for N finds on N objects.\nSo in the worst case:\nInitialize: O(N) Union: O(N) Find: O(N) Average Case 1 2 3 4 5 6 7 8 9 10 timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 251000, 1000): if int(str(i)[:2]) % 11 == 0: print(i, end=\u0026#34; \u0026#34;) qu = QuickUnion(i) st = time.time() qu.simulate() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st) simulating 1100 11100 22100 33100 44100 55100 66100 77100 88100 99100 110100 111100 112100 113100 114100 115100 116100 117100 118100 119100 220100 221100 222100 223100 224100 225100 226100 227100 228100 229100 1 2 3 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]); 1 2 3 4 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]); Worst case 1 2 3 4 5 6 7 8 9 timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 11000, 1000): print(i, end=\u0026#34; \u0026#34;) qu = QuickUnion(i) st = time.time() qu.simulate_worst_case() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st) simulating 100 1100 2100 3100 4100 5100 6100 7100 8100 9100 10100 1 2 3 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]); 1 2 3 4 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]); Quick-Union-Weighted The QuickUnion can easily be improved by trying to keep the trees balanced as the number of operations grow.\nThis can be done by keeping track of the size of the trees and using it for each union(a, b) operation. Now, instead of just changing the root of a to root of b, we compare the size of tree at a and the tree at b to decide whose root changes.\nIn the worst case now:\nInitialize: O(N) Union: O(logN) Find: O(logN) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class QuickUnionWeighted: def __init__(self, n:int=10): self.id = list(range(n)) self._sizes = [1] * n def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def root(self, ix:int) -\u0026gt; int: while ix != self.id[ix]: ix = self.id[ix] return ix def find(self, a:int, b:int) -\u0026gt; bool: return self.root(a) == self.root(b) def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.root(a) root_of_b = self.root(b) if root_of_a == root_of_b: return if self._sizes[root_of_a] \u0026gt; self._sizes[root_of_b]: self._sizes[root_of_a] += self._sizes[root_of_b] self.id[root_of_b] = root_of_a else: self._sizes[root_of_b] += self._sizes[root_of_a] self.id[root_of_a] = root_of_b def simulate(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) if random.random() \u0026gt; 0.5: st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) else: st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st) def simulate_worst_case(self) -\u0026gt; None: n = len(self.id) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) st = time.time() self.union(a, b) timings[\u0026#34;union\u0026#34;][n].append(time.time() - st) for _ in range(n): a = random.randint(0, n-1) b = random.randint(0, n-1) st = time.time() self.find(a, b) timings[\u0026#34;find\u0026#34;][n].append(time.time() - st) With balanced trees, for N Find operations over N objects, our worst time complexity should be O(NlogN).\n1 2 3 4 5 6 7 8 9 10 timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 251000, 1000): if int(str(i)[:2]) % 11 == 0: print(i, end=\u0026#34; \u0026#34;) quw = QuickUnionWeighted(i) st = time.time() quw.simulate() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st) simulating 1100 11100 22100 33100 44100 55100 66100 77100 88100 99100 110100 111100 112100 113100 114100 115100 116100 117100 118100 119100 220100 221100 222100 223100 224100 225100 226100 227100 228100 229100 1 2 3 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]); 1 2 3 4 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]); 1 2 3 4 5 6 7 8 9 10 timings = {\u0026#34;find\u0026#34;: defaultdict(list), \u0026#34;union\u0026#34;: defaultdict(list), \u0026#34;simulate\u0026#34;: []} print(\u0026#34;simulating \u0026#34;, end=\u0026#34;\u0026#34;) for i in range(100, 251000, 1000): if int(str(i)[:2]) % 11 == 0: print(i, end=\u0026#34; \u0026#34;) quw = QuickUnionWeighted(i) st = time.time() quw.simulate_worst_case() timings[\u0026#34;simulate\u0026#34;].append(time.time() - st) simulating 1100 11100 22100 33100 44100 55100 66100 77100 88100 99100 110100 111100 112100 113100 114100 115100 116100 117100 118100 119100 220100 221100 222100 223100 224100 225100 226100 227100 228100 229100 1 2 3 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;) ]); 1 2 3 4 plot([(timings[\u0026#34;find\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;find\u0026#34;].items()], \u0026#34;find\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), [np.mean(v) for t, v in timings[\u0026#34;union\u0026#34;].items()], \u0026#34;union\u0026#34;), (timings[\u0026#34;union\u0026#34;].keys(), timings[\u0026#34;simulate\u0026#34;], \u0026#34;simulate\u0026#34;) ]); Percolation Percolation is a model for many physical systems. The system can be represented in the following way:\nN-by-N grid of sites. Each site is open with probability p (or blocked with probability 1-p). System percolates if the top and bottom are connected by open sites. One example can be water flowing through a block of bricks with open sites indicating porus material. The likelihood of percolation depends on the site vacancy probability p. We will use the QuickUnionWeighted algorithm to find the threshold for p where the liklehood of percolation changes suddenly from 0 to 1.\nWe will run monte carlo simulation on a system:\nWhich starts with all sites blocked. In each step, we randomly open a site and check if the system percolates. We keep repeating the last step till the system percolates. We estimate p as (# open sites) / (N * N). Repeat the above steps M(1000x) times. If there is such a threshold for p where \u0026ldquo;the liklehood of percolation changes suddenly from 0 to 1\u0026rdquo;, then a running mean of all the p over multiple simulations should give us that number according to the law of large numbers.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class PercolationModel: def __init__(self, n:int=10): self.id = list(range(n)) self._sizes = [1] * n self._open = [0] * n self._ixs = list(range(n)) self.n = int(np.sqrt(n)) self.nxn = n def reset(self) -\u0026gt; None: self.id = list(range(self.nxn)) self._sizes = [1] * self.nxn self._open = [0] * self.nxn self._ixs = list(range(self.nxn)) def __getitem__(self, ix:int) -\u0026gt; int: return self.id[ix] def __len__(self) -\u0026gt; int: return len(self.id) def root(self, ix:int) -\u0026gt; int: while ix != self.id[ix]: ix = self.id[ix] return ix def find(self, a:int, b:int) -\u0026gt; bool: return self.root(a) == self.root(b) def is_valid_ix(self, r:int, c:int) -\u0026gt; bool: return (r \u0026gt; -1 and r \u0026lt; self.n) and (c \u0026gt; -1 and c \u0026lt; self.n) def union(self, a:int, b:int) -\u0026gt; None: root_of_a = self.root(a) root_of_b = self.root(b) if self._sizes[root_of_a] \u0026gt; self._sizes[root_of_b]: self._sizes[root_of_a] += self._sizes[root_of_b] self.id[root_of_b] = root_of_a else: self._sizes[root_of_b] += self._sizes[root_of_a] self.id[root_of_a] = root_of_b def open_site(self) -\u0026gt; None: ix = random.sample(self._ixs, 1)[0] self._ixs.remove(ix) self._open[ix] = 1 r, c = ix // self.n, ix % self.n if self.is_valid_ix(r-1, c) and self._open[ix-self.n]: # up self.union(ix, ix-self.n) if self.is_valid_ix(r, c-1) and self._open[ix-1]: # left self.union(ix, ix-1) if self.is_valid_ix(r, c+1) and self._open[ix+1]: # right self.union(ix, ix+1) if self.is_valid_ix(r+1, c) and self._open[ix+self.n]: # down self.union(ix, ix+self.n) def percolates(self) -\u0026gt; bool: last_r = self.n * (self.n-1) for c1 in range(self.n): for c2 in range(self.n): if self.find(c1, last_r+c2): return True return False def simulate(self, n_sims:int=1000, till_p=None) -\u0026gt; (float, list): p = []; for i in range(n_sims): self.reset() n_open_sites = 0 if till_p: while (n_open_sites / self.nxn) \u0026lt; till_p: self.open_site() n_open_sites += 1 p.append(n_open_sites / self.nxn) else: while not self.percolates(): self.open_site() n_open_sites += 1 p.append(n_open_sites / self.nxn) return np.mean(p), [np.mean(p[:i+1]) for i in range(n_sims)] def display(self) -\u0026gt; None: coords, colors = [], []; for r in range(pm.n): for c in range(pm.n): coords.append((r, c)) if pm._open[pm.n*r+c]: colors.append(\u0026#34;lightblue\u0026#34;) else: colors.append(\u0026#34;black\u0026#34;) plt.figure(figsize=(5, 5)) plt.scatter([c[0] for c in coords], [c[1] for c in coords], c=colors, s=740, marker=\u0026#34;s\u0026#34;) plt.xticks([]); plt.yticks([]); Let\u0026rsquo;s see how the system looks at different values of p, for different 10-by-10 grids.\n1 2 3 pm = PercolationModel(100) p, _ = pm.simulate(till_p=0.2) p 0.20000000000000004 1 pm.display() 1 2 3 pm = PercolationModel(100) p, _ = pm.simulate(till_p=0.8) p 0.8000000000000002 1 pm.display() Let\u0026rsquo;s find the p threshold.\n1 2 3 pm = PercolationModel(100) p, ps = pm.simulate(n_sims=2000) p 0.59062 1 pm.display() 1 2 plt.figure(figsize=(12, 6)) plt.plot(list(range(len(ps))), ps); 1 2 3 pm = PercolationModel(100) p, ps = pm.simulate(n_sims=5000) p 0.590384 1 pm.display() 1 2 plt.figure(figsize=(12, 6)) plt.plot(list(range(len(ps))), ps); References [1] https://www.coursera.org/learn/algorithms-part1/home/week/1.\n","date":"Apr 07","permalink":"https://iutsav.dev/posts/dynamic_connectivity_and_percolation/","tags":["python","dynamic connectivity","algorithms"],"title":"Dynamic Connectivity and Percolation"},{"categories":null,"contents":"Python provides a variety of sequences; understanding these builtin sequences saves us from reinventing the wheel. We can also aspire to create APIs that support existing and user created sequence types. Python offers two types of sequences:\nContainer sequences: These hold items of different types, for example list, tuple, etc. Flat sequences: These hold items of the same type, for example str, bytes, etc. A container sequence holds references to objects where as a flat sequence contains the value of the contents in it\u0026rsquo;s own memory. To understand the above point clearly, we need to understand how python stores objects.\nEvery python object has a header with metadata. For example, a float has:\nob_refcnt: Object\u0026rsquo;s reference count. ob_type: Object\u0026rsquo;s type. ob_fval: Object\u0026rsquo;s value. Each of these fields take 8 bytes in a 64-bit built. So an array.array of floats actually holds the raw values of the floats whereas a list of floats has several objects - the list iteself and reference to each float object. Another way the sequences can be classified is based on it\u0026rsquo;s mutability:\nMutable sequences: list, array.array, collections.deque, etc. Immutable sequences: tuple, str, bytes, etc. List Comprehensions List comprehensions or listcomps are used to create a new list from a list by transforming or/and filtering the data in the original list.\nListcomps can be created using mulltiple loops over multiple iterators as well, the key idea is to make sure that the script is readable and the transformation is obvious to the end user. It\u0026rsquo;s very easy to abuse listcomps to write incomprehensible code. We should stick to plain old for loops in such cases. Listcomps are also faster than map and filter.\n1 2 %%timeit even_squares = [x**2 for x in range(100) if x % 2 == 0] 14.6 ¬µs ¬± 47.1 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each) 1 print(even_squares) (0, 4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604) 1 2 %%timeit even_squares = list(filter(lambda x: x % 2 == 0, map(lambda x: x**2, range(100)))) 33.6 ¬µs ¬± 867 ns per loop (mean ¬± std. dev. of 7 runs, 10000 loops each) 1 print(even_squares) (0, 4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604) Generator Expressions To initialize sequences of types other than list, like arrays, tuples, etc., we can use genexp as they save memory by yielding one item at a time using the iterator protocol. A listcomp creates a whole list to feed into another constructor.\n1 2 even_squares = tuple(x**2 for x in range(100) if x % 2 == 0) print(even_squares) (0, 4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604) Tuples Beyond using tuples as \u0026ldquo;immutable lists\u0026rdquo;, they can also be thought of as a system of records. Tuples along with \u0026ldquo;unpacking\u0026rdquo; make for very readable way to access elements from a sequence by avoiding indexing.\n1 2 3 for country, capital in [(\u0026#34;India\u0026#34;, \u0026#34;New Delhi\u0026#34;), (\u0026#34;Afganistan\u0026#34;, \u0026#34;Kabul\u0026#34;), (\u0026#34;Canada\u0026#34;, \u0026#34;Ottawa\u0026#34;)]: print(f\u0026#34;{country}/{capital}\u0026#34;) India/New Delhi Afganistan/Kabul Canada/Ottawa Immutaiblity of Tuples The fact that tuples are immutable has many benefits:\nWe know that the value of a tuple object will never change. Tuples are faster than lists as they have less overheads. Tuple are faster than lists because:\nTuples can be constant folded 1 2 3 from dis import dis dis(compile(\u0026#34;(10, \u0026#39;abc\u0026#39;)\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;eval\u0026#34;)) 1 0 LOAD_CONST 0 ((10, 'abc')) 2 RETURN_VALUE 1 dis(compile(\u0026#34;[10, \u0026#39;abc\u0026#39;]\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;eval\u0026#34;)) 1 0 LOAD_CONST 0 (10) 2 LOAD_CONST 1 ('abc') 4 BUILD_LIST 2 6 RETURN_VALUE Tuples do not need to be copied 1 2 3 a = (10, 20) b = tuple(a) a is b True 1 2 3 a = [10, 20] b = list(a) a is b False Tuples do not over-allocate 1 2 3 import sys sys.getsizeof(tuple(iter(range(10)))) 120 1 sys.getsizeof(list(iter(range(10)))) 192 Here is the comment from Objects/listobject.c that explains what lists are doing:\n* This over-allocates proportional to the list size, making room * for additional growth. The over-allocation is mild, but is * enough to give linear-time amortized behavior over a long * sequence of appends() in the presence of a poorly-performing * system realloc(). * The growth pattern is: 0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ... * Note: new_allocated won\u0026#39;t overflow because the largest possible value * is PY_SSIZE_T_MAX * (9 / 8) + 6 which always fits in a size_t. Coming back to the immutability aspect of Tuples, the fact that tuples are container style sequences, they can hold mutable objects.\n1 2 3 a = (10, \u0026#34;alpha\u0026#34;, [1, 2]) b = (10, \u0026#34;alpha\u0026#34;, [1, 2]) a == b True 1 2 b[-1].append(99) a == b False 1 b (10, 'alpha', [1, 2, 99]) Examples like this can be a source of bugs. It\u0026rsquo;s always a good idea to avoid mutable objects in a tuple. A tuple without mutable objects can be used as a key for a dict. This can be verified using the hash method.\n1 hash((10, \u0026#34;alpha\u0026#34;, 1, 2)) 1614661222696736202 1 hash((10, \u0026#34;alpha\u0026#34;, [1, 2])) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-58-6818f30e4402\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 hash((10, \u0026quot;alpha\u0026quot;, [1, 2])) TypeError: unhashable type: 'list' Unpacking sequences and iterables Unpacking helps avoid indexing to extract items from sequences. It works with any iterable object, even if the object doesn\u0026rsquo;t support index notation [].\n1 2 country, capital = (\u0026#34;India\u0026#34;, \u0026#34;New Delhi\u0026#34;) country 'India' 1 capital 'New Delhi' An elegant way to use unpacking is to swap values without using a temporary variable.\n1 2 3 4 a = 10 b = 20 a, b = b, a a, b (20, 10) We can use * to grab excess items.\n1 2 a, b, *rest = range(5) a, b, rest (0, 1, [2, 3, 4]) 1 2 a, *rest, b = range(10) a, rest, b (0, [1, 2, 3, 4, 5, 6, 7, 8], 9) Unpacking works with nested structures as well.\n1 2 3 4 5 shopping_cart = [(1, \u0026#34;apple\u0026#34;, (3.2, 10)), (2, \u0026#34;milk\u0026#34;, (3, 20))] print(f\u0026#34;{\u0026#39;item\u0026#39;:\u0026lt;10} | {\u0026#39;cost\u0026#39;:\u0026gt;4} {\u0026#39;qnt\u0026#39;:\u0026gt;4} {\u0026#39;total\u0026#39;:\u0026gt;5}\\n{\u0026#39;-\u0026#39;*28}\u0026#34;) for _, item, (cost, qnt) in shopping_cart: print(f\u0026#34;{item:\u0026lt;10} | {cost:\u0026gt;4} {qnt:\u0026gt;4} {cost*qnt:\u0026gt;5}\u0026#34;) item | cost qnt total ---------------------------- apple | 3.2 10 32.0 milk | 3 20 60 Slicing Slices in python excldue the last item. This convention is common across many languages. This feature has many convenient side effects:\nLength of range(3) and my_list[:3] is the same. It\u0026rsquo;s easy to compute the length of a slice when start and stop are given using stop - start. It\u0026rsquo;s easy to split a sequence at an index x using my_list[:x] and my_list[x:] 1 2 my_list = list(range(10)) my_list[:3], my_list[3:] ([0, 1, 2], [3, 4, 5, 6, 7, 8, 9]) One uncommon use of slices is to create constants to make the code more readable. By assigning names to slices we can be more explicit instead of needing the user to figure out what the indexes mean.\n1 2 3 4 5 6 7 8 9 10 11 12 invoice = \u0026#34;\u0026#34;\u0026#34; 1 apple $3.2 10 2 banana $1.5. 5 \u0026#34;\u0026#34;\u0026#34; SKU_ID = slice(0, 2) SKU_NAME = slice(3, 9) PRICE = slice(10, 14) QUANTITY = slice(15, 17) for sku_line in invoice.split(\u0026#34;\\n\u0026#34;)[1:-1]: print(sku_line[SKU_ID], sku_line[SKU_NAME], sku_line[PRICE], sku_line[QUANTITY]) 1 apple $3.2 10 2 banana $1.5 5 The [] operator can also take multpile indexes and slices separated by commas. This pattern is common in external libraries like numpy and pandas where this notation is used to indicate index/slice across rows and columns.\n1 2 3 4 import numpy as np arr = np.array([[1, 2, 3], [4, 5, 6]]) arr array([[1, 2, 3], [4, 5, 6]]) 1 arr[:, 1:] # all rows, columns 2 and 3 array([[2, 3], [5, 6]]) Slices can also be used on the left-hand side of an assignment operation or as the target of a del operation.\n1 2 my_list = list(range(10)) my_list [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1 2 my_list[3:5] = [30, 40] my_list [0, 1, 2, 30, 40, 5, 6, 7, 8, 9] 1 2 del my_list[3:5] my_list [0, 1, 2, 5, 6, 7, 8, 9] Using * with Sequences To concatenate multiple copies of the same sequence, we multiply it by an integer which creates a new sequence.\n1 2 3 my_list = [1, 2, 3] new_list = my_list * 3 new_list [1, 2, 3, 1, 2, 3, 1, 2, 3] 1 id(my_list), id(new_list) (140325040574144, 140325040580864) Using * to multiply sequences with mutable objects can lead to weird behavior due to the fact that we end of referring to the same object.\n1 2 weird_board = [[\u0026#34;_\u0026#34;] * 3] * 3 weird_board [['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']] 1 weird_board[0] is weird_board[1] True Looking at the above(both referring to the same object), the below result should not be a surprise.\n1 2 weird_board[0][2] = \u0026#34;X\u0026#34; weird_board [['_', '_', 'X'], ['_', '_', 'X'], ['_', '_', 'X']] Augmented Assignment with Sequences The augmented assignment += works using the special method __iadd__. If __iadd__ is not implemented, Python falls back to calling __add__.\n1 2 3 my_list = [1, 2, 3] my_list += [4, 5] my_list [1, 2, 3, 4, 5] __iadd__ works similar to .extend(). If my_list does not implement __iadd__, a+=b is the same as a=a+b.\nUsage Although it\u0026rsquo;s highlighted everywhere that lists are container type sequences that can hold objects of different types, in practice we typically store objects of similar type in a list so that we can apply some common operation on them(i.e. they should all \u0026ldquo;quack\u0026rdquo; whether or not they are 100% ducks).\nTuples being used as records can be used to store objects of different types in practice.\n1 2 my_list = [1, 2, \u0026#34;30\u0026#34;, 4, \u0026#34;5\u0026#34;] my_list [1, 2, '30', 4, '5'] 1 sorted(my_list) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-135-cb83e3f42d3f\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 sorted(my_list) TypeError: '\u0026lt;' not supported between instances of 'str' and 'int' Most of the languages need us to provide a cmp(a, b) method to make comparisons to be able to sort a sequence. In python, using key argument in list.sort, max, min or sorted methods removes the need for this cmp() method. Python does compare keys but that is done in optimized C code and not using a method provided by us.\n1 sorted(my_list, key=int) [1, 2, 4, '5', '30'] 1 sorted(my_list, key=str) [1, 2, '30', 4, '5'] References [1] Luciano Ramalho. \u0026ldquo;Fluent Python\u0026rdquo;.\n","date":"Apr 05","permalink":"https://iutsav.dev/posts/python_notes_2_sequences/","tags":["python","sequences"],"title":"Python Notes, Part 2 - Sequences"},{"categories":null,"contents":"One of the best things about python is it\u0026rsquo;s consistency. Once someone has spent enough time with python and it\u0026rsquo;s ecosystem, they tend to adapt easily to new tools, modules as long as they are written in a pythonic way.\nWhat is considered pythonic is a very subjective matter, but we can always look at the \u0026ldquo;Zen of Python\u0026rdquo; for some guidance.\n1 import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! Special Methods (dunder methods) The heart of what makes an object pythonic is the Python Data Model. Python Data Model provides apis that help us create consistent behavior for our python objects. We achieve this consistency by adding special methods called dunder methods(or double underscore methods) to our objects.\nBefore looking into dunder methods, lets look at a simple object to demonstrate a point about python objects and the consistency we keep talking about.\nTypically in Object Oriented Design we define a class which encapsulates some data and the associated methods that act on the data.\nSuppose we have an object which internally maintains a collection and we want to know the length of the collection, typically the OOO design will go something like\nMyObject.len() or MyObject.size() ... whether it\u0026rsquo;s len(), length(), size() or something else is the choice of the designer of this API and the users need to read the documentation/specification before using the object.\nIn python, we do this by calling len(MyObject). Now this is not a typical OOP behaviour, but this is what is magical about python. Under the hood, len() refers to the __len__ dunder method of the object. By adding these special methods to our class definition, we can provide consistent behaviour to our objects so that new users can start using them easily and in a predictable manner(as long as they expect this behaviour).\nPython has many more dunder methods. These methods provide a framework for building almost everything in python, be it iterators, collections, string representations, classes, coroutines, etc.\nLet\u0026rsquo;s look at more dunder methods using the following example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from dataclasses import dataclass import random @dataclass class DiePair: die1: int die2: int class TwoDieRoll: def __init__(self): self._pairs = [DiePair(die1=i, die2=j) for i in range(1, 7) for j in range(1, 7)] def __getitem__(self, ix: int): return self._pairs[ix] def __len__(self): return len(self._pairs) def roll(self): return random.choice(self._pairs) 1 2 3 two_dies = TwoDieRoll() len(two_dies) 36 1 two_dies[10] DiePair(die1=2, die2=5) Beyond indexing, by adding __getitem__ we get to access to other features like slicing\n1 two_dies[:5] [DiePair(die1=1, die2=1), DiePair(die1=1, die2=2), DiePair(die1=1, die2=3), DiePair(die1=1, die2=4), DiePair(die1=1, die2=5)] 1 two_dies[-5:] [DiePair(die1=6, die2=2), DiePair(die1=6, die2=3), DiePair(die1=6, die2=4), DiePair(die1=6, die2=5), DiePair(die1=6, die2=6)] Our object two_dies is also an iterable now\n1 2 for die_pair in two_dies: print(die_pair) DiePair(die1=1, die2=1) DiePair(die1=1, die2=2) DiePair(die1=1, die2=3) DiePair(die1=1, die2=4) DiePair(die1=1, die2=5) DiePair(die1=1, die2=6) DiePair(die1=2, die2=1) DiePair(die1=2, die2=2) DiePair(die1=2, die2=3) ... Iteration is often implicit, for example\n1 DiePair(1, 5) in two_dies True 1 DiePair(1, 8) in two_dies False Both the above example work without the __contains__ dunder method in our class. In it\u0026rsquo;s absence, the in operator does a sequential scan.\nWe can use built in modules to add methods like roll() to our class.\n1 two_dies.roll() DiePair(die1=3, die2=1) We can also sort the dies in reverse order based on die2.\n1 sorted(two_dies, key=lambda die: die.die2, reverse=True) [DiePair(die1=1, die2=6), DiePair(die1=2, die2=6), DiePair(die1=3, die2=6), DiePair(die1=4, die2=6), DiePair(die1=5, die2=6), DiePair(die1=6, die2=6), DiePair(die1=1, die2=5), DiePair(die1=2, die2=5), DiePair(die1=3, die2=5), ...] All this is possible because of __getitem__ which makes our object behave like a python collection.\nDunder methods are not called directly by the user, but internally by the interpreter, except in special cases like metaprogramming or in the __init__ of a class to initialize the base class it\u0026rsquo;s deriving from.\nThat does not mean we can\u0026rsquo;t invoke them explicitly.\n1 two_dies.__len__() 36 More uses of dunder methods Dunder methods have lots of other uses as well. For example\nEmulating numeric types(operator overloading) String representation for objects Emulating Numeric Types Let\u0026rsquo;s create a class that allows us to create vectors we use in physics or to represent data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import math class Vector: def __init__(self, x: int, y: int): self.x = x self.y = y def __repr__(self): return f\u0026#34;Vector({self.x}, {self.y})\u0026#34; def __add__(self, other_vector): return Vector(self.x + other_vector.x, self.y + other_vector.y) def __mul__(self, scalar: int): return Vector(scalar * self.x, scalar * self.y) def __abs__(self): return math.hypot(self.x, self.y) By adding __add__ and __mul__ special methods, we can start adding two vectors and multiplying scalars to our vectors like we expect.\n1 2 vector1 = Vector(1, 2) vector2 = Vector(10, 3) 1 vector1 + vector2 Vector(11, 5) 1 vector2 * 3 Vector(30, 9) 1 abs(Vector(3, 4)) 5.0 String representation of objects The __repr__ method allows us to create a string representation for the object. This is useful for debugging/logging purposes. If possible, it should return a string that describes exactly how the object is created.\n1 vector1 Vector(1, 2) 1 vector2 Vector(10, 3) We also have a __str__ method that returns a string describing the object. Typically this is a description of the object for the end user.\nWhen the __repr__ is descriptive enough, the __str__ method can be avoided as it defaults to __repr__.\nFrom stackoverflow:\nMy rule of thumb: repr is for developers, str is for customers.\nCollection API The UML diagram shows the three essential interface every collection should implement\nIterable Sized Container Three very important specializations of collection are\nSequence: formalizing the interface of built-ins like list and str; Mapping: implemented by dict, collections.defaultdict, etc.; Set: the interface of the set and frozenset built-in types. For example, let\u0026rsquo;s create a list\n1 2 my_list = [1, 2, 3] dir(my_list) ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort'] The dir() method shows all the methods and special methods available for an object. Here we can see all the essential interfacs as well as others. The built in python collections implement these special methods for us.\nLet\u0026rsquo;s look at a set now.\n1 2 my_set = {1, 2, 3} dir(my_set) ['__and__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__iand__', '__init__', '__init_subclass__', '__ior__', '__isub__', '__iter__', '__ixor__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__rand__', '__reduce__', '__reduce_ex__', '__repr__', '__ror__', '__rsub__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__xor__', 'add', 'clear', 'copy', 'difference', 'difference_update', 'discard', 'intersection', 'intersection_update', 'isdisjoint', 'issubset', 'issuperset', 'pop', 'remove', 'symmetric_difference', 'symmetric_difference_update', 'union', 'update'] 1 {1, 2, 3} - {1, 2} {3} We are able to do the above operation because of the special method __sub__ that\u0026rsquo;s been provided to us by default.\nReferences [1] Luciano Ramalho. \u0026ldquo;Fluent Python\u0026rdquo;.\n[2] https://docs.python.org/3/reference/datamodel.html.\n","date":"Mar 05","permalink":"https://iutsav.dev/posts/python_notes_1_datamodel/","tags":["python","data model"],"title":"Python Notes, Part 1 - Data Model"},{"categories":null,"contents":"We will use FastAPI to serve our trained models behind a REST endpoint. FastAPI is a web framework built on top of Starlette. Scripts related to the APIs are located at MLOps/app.\nThe directory structure is:\n1 2 3 4 5 6 7 ‚îú‚îÄ‚îÄ application.py # FastAPI app and launcher uvicorn.run ‚îú‚îÄ‚îÄ models # pydantic model for reponse validation ‚îÇ¬†‚îî‚îÄ‚îÄ predict.py # pydantic model for predict endpoint ‚îî‚îÄ‚îÄ routes # to maintain larger applications ‚îú‚îÄ‚îÄ endpoints ‚îÇ¬†‚îî‚îÄ‚îÄ predict.py # predict endpoint ‚îî‚îÄ‚îÄ router.py # APIRouter module to maintain larger apps Though we have used the APIRouter module here, it\u0026rsquo;s really needed when we have lots of API endpoints in a larger app.\nLook at https://fastapi.tiangolo.com/tutorial/bigger-applications/.\nModel Artifacts To use the model to get predictions, we need to load all the model related artifacts.\nWe load the trained model directly using the mlflow.sklearn module. Here we load the version number 1 of our sk-learn-naive-bayes-clf-model model.\n1 2 3 4 5 6 7 8 9 10 11 # manually pick the model version from trained models sk_model = mlflow.sklearn.load_model(model_uri=\u0026#34;models:/sk-learn-naive-bayes-clf-model/1\u0026#34;) # mlflow does not store data manipulation routines like label encoding # we need to manage the LabelEncoder and TfidfVectorizer ourselves with open(BASE_DIR / \u0026#34;artifacts/target_encoder.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: target_encoder = pickle.load(f) with open(BASE_DIR / \u0026#34;artifacts/vectorizer.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: vectorizer = pickle.load(f) logger.info(\u0026#34;Loaded model artifacts\u0026#34;) Beyond the trained classifier, we also need the TfidfVectorizer to vectorize the text and LabelEncoder to map the predictions to actual labels.\nThese artifacts are not saved/managed by MLflow as it only mangages the artifacts realted to the ML algorithm. #\nExclude certain preprocessing \u0026amp; feature manipulation estimators from patching. These estimators represent data manipulation routines (e.g., normalization, label encoding) rather than ML algorithms. Accordingly, we should not create MLflow runs and log parameters / metrics for these routines, unless they are captured as part of an ML pipeline (via sklearn.pipeline.Pipeline).\n1 2 3 4 5 6 7 8 9 10 11 # manually pick the model version from trained models sk_model = mlflow.sklearn.load_model(model_uri=\u0026#34;models:/sk-learn-naive-bayes-clf-model/1\u0026#34;) # mlflow does not store data manipulation routines like label encoding # we need to manage the TfidfVectorizer and TfidfVectorizer ourselves with open(BASE_DIR / \u0026#34;artifacts/target_encoder.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: target_encoder = pickle.load(f) with open(BASE_DIR / \u0026#34;artifacts/vectorizer.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: vectorizer = pickle.load(f) logger.info(\u0026#34;Loaded model artifacts\u0026#34;) Predict Endpoint # Let\u0026rsquo;s look at the /predict endpoint.\n1 2 3 4 5 6 7 8 9 10 @router.post(\u0026#34;/predict\u0026#34;) async def predit(text: str) -\u0026gt; PredictResponseModel: logger.info(f\u0026#34;Received text for prediction: {text}\u0026#34;) processed_text_list = preprocess_text([text]) x = vectorizer.transform(processed_text_list) pred = sk_model.predict_proba(x) mapped_pred = dict(zip(target_encoder.classes_, pred[0])) logger.info(f\u0026#34;Prediction: {mapped_pred}\u0026#34;) return PredictResponseModel(preds=mapped_pred).preds Here, we preprocess the text using preprocess_text(), vectorize it using vectorizer.transform() and finally generate predictions using the classifier sk_model.predict_proba().\nWe then map the probabilities to the actual labels(target_encoder.classes_) and return the predictions by wrapping around the PredictResponseModel pydantic model for data validation.\nThe API can be accessed at http://127.0.0.1:8010/docs.\nElasticAPM We have also added ElasticAPM as a middleware for monitoring our FastAPI application. #\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import uvicorn from elasticapm.contrib.starlette import ElasticAPM, make_apm_client from fastapi import FastAPI from loguru import logger from mlops.app.routes.router import api_router def get_fastapi_application() -\u0026gt; FastAPI: application = FastAPI(title=\u0026#34;MLOps\u0026#34;) application.add_middleware( ElasticAPM, client=make_apm_client({\u0026#34;SERVICE_NAME\u0026#34;: \u0026#34;MLOps Example\u0026#34;}) ) application.include_router(api_router) return application app = get_fastapi_application() if __name__ == \u0026#34;__main__\u0026#34;: logger.info(\u0026#34;*** Starting Prediction Server ***\u0026#34;) uvicorn.run(app, host=\u0026#34;127.0.0.1\u0026#34;, port=8010) Once we start using the /predict endpoint, we can head over to http://localhost:5601/ to look at the app related metrics like Latency, Throughput etc, and system level metrics like CPU usage and System memory usage.\nUnder the hood, it uses Elastic Search, Kibana and an APM server launched using the docker-compose-monitoring.yaml during the setup.\nThe dashboard also provides a trace for each request.\n","date":"Nov 19","permalink":"https://iutsav.dev/posts/mlops_template_4_fastapi_elasticapm/","tags":["elasticapm","fastapi"],"title":"MLOps Template, Part 4 - FastAPI + ElasticAPM"},{"categories":null,"contents":"In the last part, we saw our pipeline at the end. Steps 1-5 deal with data manipulation. We train our model in step 6 and that\u0026rsquo;s where MLflow comes into picture.\n1 2 3 4 5 6 7 8 9 10 11 12 13 @pipeline def ml_pipeline(): # 1. fetch training data texts, target = get_training_dataset() # 2. minimal text preprocessing # 3. tfidf vectorization vectorizer, X = get_vectorizer_and_features(preprocess_text(texts)) # 4. target encoding target_encoder, encoded_target = get_targetencoder_and_encoded_targets(target) # 5. train test split X_train, X_test, y_train, y_test = train_test_split(X, encoded_target) # 6. model training, validation, registry, artifact storage train_clf(X_train, X_test, y_train, y_test) MLflow # MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It tackles four primary functions:\nTracking experiments to record and compare parameters and results (MLflow Tracking).\nPackaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production (MLflow Projects).\nManaging and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\nProviding a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\nFor our usecase, we mainly focus on Mlflow Tracking and Mlflow Registry. train_clf() internally calls train_and_validate_clf() located at mlops/ml_workflow/naive_bayes_clf.py.\nThis script has as all the MLflow components and we will look at those in detail.\nTracking # 1 2 3 4 5 6 7 8 9 10 11 12 # setting env vars for minio artifact storage set_env_vars() mlflow.set_tracking_uri(os.getenv(\u0026#34;MLFLOW_TRACKING_URI\u0026#34;)) # creates a new mlflow experiment MLFLOW_EXPERIMENT_NAME if it doesn\u0026#39;t exist exps = [exp.name for exp in mlflow.tracking.MlflowClient().list_experiments()] if not os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;) in exps: mlflow.create_experiment( os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;), artifact_location=os.getenv(\u0026#34;MLFLOW_ARTIFACT_LOCATION\u0026#34;), ) MLflow runs can be recorded to local files, to a SQLAlchemy compatible database, or remotely to a tracking server. By default, the MLflow Python API logs runs locally to files in an mlruns directory wherever you ran your program. You can then run mlflow ui to see the logged runs.\nTo log runs remotely, set the MLFLOW_TRACKING_URI environment variable to a tracking server‚Äôs URI or call mlflow.set_tracking_uri().\nThere are different kinds of remote tracking URIs:\nLocal file path (specified as file:/my/local/dir), where data is just directly stored locally. Database encoded as \u0026lt;dialect\u0026gt;+\u0026lt;driver\u0026gt;://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt;. MLflow supports the dialects mysql, mssql, sqlite, and postgresql. For more details, see SQLAlchemy database uri. HTTP server (specified as https://my-server:5000), which is a server hosting an MLflow tracking server. Databricks workspace (specified as databricks or as databricks://\u0026lt;profileName\u0026gt;, a Databricks CLI profile. Refer to Access the MLflow tracking server from outside Databricks [AWS] [Azure], or the quickstart to easily get started with hosted MLflow on Databricks Community Edition. We use sqlite for tracking. MLFLOW_TRACKING_URI=sqlite:///mlflow.db. This file stays at the root directory of the project.\nYou can optionally organize runs into experiments, which group together runs for a specific task. You can create an experiment using the mlflow experiments CLI, with mlflow.create_experiment(), or using the corresponding REST parameters. The MLflow API and UI let you create and search for experiments.\n1 2 3 4 5 6 7 8 9 10 11 12 # setting env vars for minio artifact storage set_env_vars() mlflow.set_tracking_uri(os.getenv(\u0026#34;MLFLOW_TRACKING_URI\u0026#34;)) # creates a new mlflow experiment MLFLOW_EXPERIMENT_NAME if it doesn\u0026#39;t exist exps = [exp.name for exp in mlflow.tracking.MlflowClient().list_experiments()] if not os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;) in exps: mlflow.create_experiment( os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;), artifact_location=os.getenv(\u0026#34;MLFLOW_ARTIFACT_LOCATION\u0026#34;), ) First time we execute the pipeline, experiment MLFLOW_EXPERIMENT_NAME=MLOps is created and is used for subsequent runs.\n1 2 3 4 5 6 7 8 def train_and_validate_clf( X_train: np.array, X_test: np.array, y_train: np.array, y_test: np.array ) -\u0026gt; str: mlflow.set_experiment(os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;)) with mlflow.start_run(run_name=\u0026#34;NAIVE_BAYES_CLF\u0026#34;): # ignoring rest of the script pass Registry # Under an experiment, we can log metrics, log model parameters, save and version model artifacts. These steps come under the purview of model-registry. Here, we log\nclassifier parameter alpha classification metrics precision, recall and f1_score We also save and version the model using mlflow.sklearn.log_model. Each subsequent run of the pipeline with the same artifact_path and registered_model_name increases the model version by 1 in the registry.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def train_and_validate_clf( X_train: np.array, X_test: np.array, y_train: np.array, y_test: np.array ) -\u0026gt; str: mlflow.set_experiment(os.getenv(\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;)) with mlflow.start_run(run_name=\u0026#34;NAIVE_BAYES_CLF\u0026#34;): clf = MultinomialNB() mlflow.log_param(\u0026#34;alpha\u0026#34;, clf.get_params()[\u0026#34;alpha\u0026#34;]) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) scores = precision_recall_fscore_support(y_test, y_pred, average=\u0026#34;weighted\u0026#34;) mlflow.log_metric(\u0026#34;precision\u0026#34;, scores[0]) mlflow.log_metric(\u0026#34;recall\u0026#34;, scores[1]) mlflow.log_metric(\u0026#34;f1_score\u0026#34;, scores[2]) mlflow.sklearn.log_model( sk_model=clf, artifact_path=\u0026#34;naive-bayes-model\u0026#34;, registered_model_name=\u0026#34;sk-learn-naive-bayes-clf-model\u0026#34;, ) return classification_report(y_test, y_pred) Heading over to the experiment section at http://127.0.0.1:5000, we can see the registry entry for each of our runs, along with the logged metrics and model parameters.\nFor each run, we can also look at the artifact location.\nWhen we created the experiment, we speficified artifact_location=os.getenv(\u0026quot;MLFLOW_ARTIFACT_LOCATION\u0026quot;) where MLFLOW_ARTIFACT_LOCATION=s3://mlflow. Internally, this stores the model artifacts locally at the directory minio_data we created during the setup.\nWe can head over to http://127.0.0.1:9001/buckets/mlflow/browse to look at the saved model artifacts.\nIn the next part, we will look at serving the trained model being a REST endpoint.\n","date":"Nov 05","permalink":"https://iutsav.dev/posts/mlops_template_3_mlflow_minio/","tags":["minio","mlflow"],"title":"MLOps Template, Part 3 - MLflow + MinIO"},{"categories":null,"contents":"Here we will cover our small pipeline written in dagster.\nDagster is a data orchestrator for machine learning, analytics, and ETL\nDagster is a second generation data orchestrator that focues on being data-driven rather than task-driven(like Airflow).\nSolids and Pipelines # Dagster\u0026rsquo;s core abstractions are solids and pipelines.\nSolids are individual units of computation that we wire together to form pipelines. By default, all solids in a pipeline execute in the same process. In production environments, Dagster is usually configured so that each solid executes in its own process.\nDataset Our pipeline will operate on Question Type Classification. This data helps to classify the given Questions into respective categories based on what type of answer it expects such as a numerical answer or a text description or a place or human name etc.\nQuestion Category How did serfdom develop in and then leave Russia ? DESCRIPTION What films featured the character Popeye Doyle ? ENTITY How can I find a list of celebrities \u0026rsquo; real names ? DESCRIPTION What fowl grabs the spotlight after the Chinese Year of the Monkey ? ENTITY What is the full form of .com ? ABBREVIATION What contemptible scoundrel stole the cork from my lunch ? HUMAN What team did baseball \u0026rsquo;s St. Louis Browns become ? HUMAN Category can take following values - HUMAN, ENTITY, DESCRIPTION, NUMERIC, LOCATION.\nSolid # A solid is a unit of computation in a data pipeline. Typically, you\u0026rsquo;ll define solids by annotating ordinary Python functions with the @solid decorator.\nOur first solid get_training_dataset loads the data from csv and returns\ntexts (List[str]) - Question target (List[str]) - Category 1 2 3 4 5 6 7 8 9 10 11 12 @solid( output_defs=[ OutputDefinition(name=\u0026#34;texts\u0026#34;, is_required=True), OutputDefinition(name=\u0026#34;target\u0026#34;, is_required=True), ] ) def get_training_dataset(context): texts, target = dataloaders.get_input_dataset(INPUT_DATASET_LOC) context.log.info(f\u0026#34;Loaded data; N={len(texts)}, Targets={set(target)}\u0026#34;) yield Output(texts, \u0026#34;texts\u0026#34;) yield Output(target, \u0026#34;target\u0026#34;) OutputDefinition # To define multiple outputs, or to use a different output name than \u0026ldquo;result\u0026rdquo;, you can provide OutputDefinitions to the @solid decorator.\nWhen you have more than one output, you must yield an instance of the Output class to disambiguate between outputs.\nLet\u0026rsquo;s look at another solid preprocess_text.\n1 2 3 4 5 @solid def preprocess_text(context, texts): texts = text_preprocessing.preprocess_text(texts) context.log.info(f\u0026#34;Text pre-processing done; N={len(texts)}\u0026#34;) return texts Here, we are doing some text preprocessing. Since we are not returning mulitple outputs here, we can avoid OutputDefinition.\nPipeline # To execute our solid, we\u0026rsquo;ll embed it in an equally simple pipeline. A pipeline is a set of solids arranged into a DAG of computation. You\u0026rsquo;ll typically define pipelines by annotating ordinary Python functions with the @pipeline decorator.\nLet\u0026rsquo;s look at our Pipeline.\n1 2 3 4 5 6 7 8 9 10 11 12 13 @pipeline def ml_pipeline(): # 1. fetch training data texts, target = get_training_dataset() # 2. minimal text preprocessing # 3. tfidf vectorization vectorizer, X = get_vectorizer_and_features(preprocess_text(texts)) # 4. target encoding target_encoder, encoded_target = get_targetencoder_and_encoded_targets(target) # 5. train test split X_train, X_test, y_train, y_test = train_test_split(X, encoded_target) # 6. model training, validation, registry, artifact storage train_clf(X_train, X_test, y_train, y_test) Here we call few solids like get_training_dataset, preprocess_text, get_vectorizer_and_features, etc.\nThese calls don\u0026rsquo;t actually execute the solids. Within the bodies of functions decorated with @pipeline, we use function calls to indicate the dependency structure of the solids making up the pipeline.\nExecuting Our First Pipeline # Dagit to visualize our pipeline in Dagit, from the directory in which we have saved the pipeline file.\n1 2 $ dagit -f mlops/pipeline.py # Loading repository... Serving on http://127.0.0.1:3000 We can head over to the browser and look at the pipeline.\nTo execute the pipeline from the UI, head over to the playground section and click Launch Execution (at the bottom right).\nIf the pipeline executed successfully, you should see logs like this.\nThe pipeline we just executed registered our trained model using MLflow and stored the model artifacts using minio.\nWe will look at those components in the next part.\n","date":"Oct 26","permalink":"https://iutsav.dev/posts/mlops_template_2_dagster/","tags":["dagster"],"title":"MLOps Template, Part 2 - Dagster Pipeline"},{"categories":null,"contents":"In this series of posts, we will take a small dataset and go through various steps like building Data pipelines, ML workflow management, API development and Monitoring.\nThese steps are necessary for operationalization of any machine-learning based model.\nThese posts are in no way exhaustive in covering the breadth of MLOps.Several key pieces like the CI/CD pipeline, monitoring for drift, etc are missing at the moment, which might get added later.\nStack We will be using the following tools in this project\nData Pipeline: Dagster ML registry: MLflow API Development: FastAPI Monitoring: ElasticAPM Dev Setup Poetry # 1 2 3 $ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - $ poetry --version # Poetry version 1.1.10 pre-commit # 1 2 3 $ pip install pre-commit $ pre-commit --version # pre-commit 2.15.0 Minio Follow the instructions here - Minio installation.\nFor Mac users\n1 brew install minio/stable/minio Install python packages 1 2 3 4 5 $ poetry install # Installing dependencies from lock file # No dependencies to install or update # Installing the current project: mlops (0.1.0) MLflow 1 2 3 4 5 6 7 8 9 10 $ poetry shell $ export MLFLOW_S3_ENDPOINT_URL=http://127.0.0.1:9000 $ export AWS_ACCESS_KEY_ID=minioadmin $ export AWS_SECRET_ACCESS_KEY=minioadmin # make sure that the backend store and artifact locations are same in the .env file as well $ mlflow server \\ --backend-store-uri sqlite:///mlflow.db \\ --default-artifact-root s3://mlflow \\ --host 0.0.0.0 MinIO 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ export MINIO_ROOT_USER=minioadmin $ export MINIO_ROOT_PASSWORD=minioadmin $ mkdir minio_data $ minio server minio_data --console-address \u0026#34;:9001\u0026#34; # API: http://192.168.29.103:9000 http://10.119.80.13:9000 http://127.0.0.1:9000 # RootUser: minioadmin # RootPass: minioadmin # Console: http://192.168.29.103:9001 http://10.119.80.13:9001 http://127.0.0.1:9001 # RootUser: minioadmin # RootPass: minioadmin # Command-line: https://docs.min.io/docs/minio-client-quickstart-guide # $ mc alias set myminio http://192.168.29.103:9000 minioadmin minioadmin # Documentation: https://docs.min.io Go to http://127.0.0.1:9001/buckets/ and create a bucket called mlflow.\nDagster 1 2 $ poetry shell $ dagit -f mlops/pipeline.py ElasticAPM 1 $ docker-compose -f docker-compose-monitoring.yaml up FastAPI 1 2 3 $ poetry shell $ export PYTHONPATH=. $ python mlops/app/application.py ","date":"Oct 22","permalink":"https://iutsav.dev/posts/mlops_template_1_setup/","tags":["mlflow","minio","fastapi","elasticapm","dagster"],"title":"MLOps Template, Part 1 - Setup"},{"categories":null,"contents":"In this post, we will train a Word2Vec skip-gram model from scratch on some text and inspect the trained embeddings at the end.\nThe first step for using any kind of NLP pipeline is to vectorize the text. Traditionally, we used to do this using one-hot representation of vectors. This had various downsides like:\nVectors tend to be very long as their size depends on the vocabulary size of the corpus(which grows with the corpus size). They don\u0026rsquo;t have any understanding of the text. They are sparse and can\u0026rsquo;t be used for any comparison as any two one-hot encoded vector from a set will be orthogonal. Word2vec is able to tackle all these challenges. The architecture we define later will enable us to learn a distributed representation for each word/token in our corpus. The key idea being, we can represent a word by adding contextual information to it. Context refers to the words/tokens neighbouring the word/token of interest to us.\nOf course this technique is not perfect and has it\u0026rsquo;s own downsides. The key downside being we loose the word/token ordering while training the word2vec model, hence we are essentially dealing with a bag of words model. This makes the resulting embeddings not suitable for sentence level representations. We will cover other techniques later (RNNs, Transformers) that produce embeddings more suitable for sentences. For now our sole focus will be learning good word/token level embeddings.\nFirst we will import the necessary libraries 1 2 3 4 5 6 7 8 9 10 from collections import Counter import re from typing import List import numpy as np import requests from sklearn.metrics.pairwise import cosine_similarity import torch import torch.nn as nn from torch.utils.data import DataLoader, Dataset The whole process of training the embeddings can be broken down into the following key steps:\nWe define a class TextProcessor that has methods to pre-process the text. fetch_text() fetches the texts directly from urls containing our text data. process_text() applies basic pre-procesing steps to the text and creates tokens using white space tokenization. prepare_vocab() creates token to index mapping and vice versa. Create torch Dataset and Dataloader where each index in the dataset will be a tuple, (center_word_ix, context_word_ix); i.e. pairs of indices of center word and context based on a pre defined window size. The window side determines the number of context words around a center word. For example, window size of 5 will create the follwing center and context pairs for the sentence \u0026ldquo;a quick brown fox jumps\u0026rdquo; with \u0026ldquo;brown\u0026rdquo; being the center word - [(\u0026quot;brown\u0026quot;, \u0026quot;a\u0026quot;), (\u0026quot;brown\u0026quot;, \u0026quot;quick\u0026quot;), (\u0026quot;brown\u0026quot;, \u0026quot;fox\u0026quot;), (\u0026quot;brown\u0026quot;, \u0026quot;jumps\u0026quot;)] Create our SkipGramModel model by defining a custom model on top of the torch.nn.Module. Define our loss function, optimizer and scheduler. Train the model for a certain number of epochs. Finally, we will create a similarity matrix(cosine similarity) using the trained embeddings and explore similar embeddings for some word query.\nFetching and preparing the data 1 2 3 4 5 6 7 8 9 10 # We will be using the first four Harry Potter books. BASE_URL = \u0026#34;https://raw.githubusercontent.com/formcept/\u0026#34; BASE_URL += \u0026#34;whiteboard/master/nbviewer/notebooks/data/harrypotter\u0026#34; BOOK_URLS = [ f\u0026#34;{BASE_URL}/Book%201%20-%20The%20Philosopher\u0026#39;s%20Stone.txt\u0026#34;, f\u0026#34;{BASE_URL}/Book%202%20-%20The%20Chamber%20of%20Secrets.txt\u0026#34;, f\u0026#34;{BASE_URL}/Book%203%20-%20The%20Prisoner%20of%20Azkaban.txt\u0026#34;, f\u0026#34;{BASE_URL}/Book%204%20-%20The%20Goblet%20of%20Fire.txt\u0026#34; ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class TextProcessor: def __init__(self, urls: List[str]): self.urls = urls def fetch_text(self): self.text = \u0026#34;\u0026#34; for i, url in enumerate(self.urls): r = requests.get(url) self.text += \u0026#34;\\n\\n\\n\u0026#34; + r.text print(f\u0026#34;Fetched {i+1}/{len(self.urls)} urls ...\u0026#34;) def process_text(self): print(\u0026#34;Processing text ...\u0026#34;) sentences = re.split(\u0026#34;\\n{2,}\u0026#34;, self.text) print(f\u0026#34;{len(sentences)} sentences\u0026#34;) self.clean_sentences = [] for txt in sentences: txt = txt.strip().lower() txt = txt.replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) txt = \u0026#34; \u0026#34;.join(re.findall(\u0026#34;\\w+\u0026#34;, txt)) if txt: self.clean_sentences.append(txt) # Tokens and token counter will be used later to create a vocabulary for # us which will be used to map words/tokens to indices and vice versa, to # create training data and recreate the texts from predictions respectively. print(f\u0026#34;{len(sentences)} filtered sentences\u0026#34;) tokens = \u0026#34; \u0026#34;.join(self.clean_sentences).split(\u0026#34; \u0026#34;) print(f\u0026#34;{len(tokens)} tokens\u0026#34;) self.token_counter = Counter(tokens) print(f\u0026#34;{len(self.token_counter)} unique tokens\u0026#34;) def prepare_vocab(self, min_count: int): # min_count is the minimum number of times a token should appear in the # text to be considered in the vocabulary else they are assigned to a # default index which is equal to `len(vocab)`. self.w2ix = {} for i, (token, count) in enumerate( self.token_counter.most_common(len(self.token_counter)) ): if count \u0026lt; min_count: break else: self.w2ix[token] = i self.ix2w = {ix: w for w, ix in self.w2ix.items()} # Assign default index to rest of the tokens n = len(self.w2ix) for token, _ in self.token_counter.most_common(len(self.token_counter)): if token not in self.w2ix: self.w2ix[token] = n self.ix2w[n] = \u0026#34;\u0026lt;unk\u0026gt;\u0026#34; self.vocab_size = n + 1 print(f\u0026#34;Vocab size: {self.vocab_size}\u0026#34;) 1 2 3 4 text_processor = TextProcessor(BOOK_URLS) text_processor.fetch_text() text_processor.process_text() text_processor.prepare_vocab(min_count=20) Fetched 1/4 urls ... Fetched 2/4 urls ... Fetched 3/4 urls ... Fetched 4/4 urls ... Processing text ... 20033 sentences 20033 filtered sentences 501854 tokens 15078 unique tokens Vocab size: 2103 Preparing the torch Dataset and Dataloader The Dataloader will enable us to send data in batches during the forward pass of the model training.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class HarryPotterDataset(Dataset): def __init__(self, text_processor: TextProcessor, window_size: int=3): self.data = [] self.text_processor = text_processor self._init_data() def _init_data(self): for txt in text_processor.clean_sentences: splits = txt.split(\u0026#34; \u0026#34;) for i in range(window_size, len(splits) -1): center_word = splits[i] window_words = splits[i-window_size:i]+splits[i+1:i+window_size+1] for context in window_words: # Each data point under self.data will be a tuple with index 0 # containing the index(w2ix) for center_word and # index 1 containing the index(w2ix) for context_word self.data.append( (text_processor.w2ix[center_word], text_processor.w2ix[context]) ) def __getitem__(self, ix: int): return self.data[ix] def __len__(self): return len(self.data) 1 2 3 dataset = HarryPotterDataset(text_processor, 3) dataloader = DataLoader(dataset, batch_size=64, shuffle=True) len(dataset) 2489461 Creating the Word2Vec model using torch.nn.Module 1 2 3 4 5 6 7 8 class SkipGramModel(nn.Module): def __init__(self, vocab_size: int, embedding_size: int=50): super().__init__() self.embedding = nn.Embedding(vocab_size, embedding_size) self.linear = nn.Linear(embedding_size, vocab_size) def forward(self, x): return self.linear(self.embedding(x)) The model architecure is Embedding -\u0026gt; Linear -\u0026gt; Softmax.\nEmbedding is nothing but \u0026ldquo;A simple lookup table that stores embeddings of a fixed dictionary and size\u0026rdquo; - pytorch documentation.\nBased on our model architecture, this layer will contain the trained token/word embeddings of interest to us and we will discard the weight matrix from the Linear layer.\nIn each forward pass we:\nPass indices of center word as input x. These indices are looked up in the self.embedding table. We transform the vector from last step using the Linear layer to get another vector with dimension vocab_size. Finally we apply softmax and calcualte loss. A trained model should output high values for the context indices for a given center word.\n1 2 device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = SkipGramModel(text_processor.vocab_size, embedding_size=30).to(device) Defining our loss function, optimizer and scheduler 1 2 3 criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.005) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) 1 2 N_EPOCHS = 10 STEP_TO_LOG = 10000 Training the model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 epoch_losses, batch_losses = [], [] for epoch in range(N_EPOCHS): losses = [] for i, data in enumerate(dataloader): inputs, outputs = data[0].long().to(device), data[1].long().to(device) pred = model(inputs) loss = criterion(pred, outputs) losses.append(loss.item()) loss.backward() optimizer.step() optimizer.zero_grad() if i % STEP_TO_LOG == 0: print(f\u0026#34;epoch {epoch + 1}; steps={i:\u0026gt;5}/{len(dataloader)}; loss={loss:\u0026lt;.5f}\u0026#34;) scheduler.step() batch_losses += losses epoch_losses.append(np.mean(losses)) print(f\u0026#34;epoch {epoch + 1} ; lr={scheduler._last_lr[0]}; loss={epoch_losses[-1]:\u0026lt;.5f}\\n\u0026#34;) epoch 1; steps= 0/38898; loss=7.80020 epoch 1; steps=10000/38898; loss=5.55375 epoch 1; steps=20000/38898; loss=5.58139 epoch 1; steps=30000/38898; loss=5.27452 epoch 1 ; lr=0.005; loss=5.60440 epoch 2; steps= 0/38898; loss=4.85012 epoch 2; steps=10000/38898; loss=5.81531 epoch 2; steps=20000/38898; loss=5.07261 epoch 2; steps=30000/38898; loss=5.62864 epoch 2 ; lr=0.005; loss=5.52841 epoch 3; steps= 0/38898; loss=5.27015 epoch 3; steps=10000/38898; loss=5.55336 epoch 3; steps=20000/38898; loss=5.67527 epoch 3; steps=30000/38898; loss=5.60372 epoch 3 ; lr=0.005; loss=5.52233 epoch 4; steps= 0/38898; loss=5.50681 epoch 4; steps=10000/38898; loss=5.48357 epoch 4; steps=20000/38898; loss=5.18897 epoch 4; steps=30000/38898; loss=5.74382 epoch 4 ; lr=0.005; loss=5.52111 epoch 5; steps= 0/38898; loss=5.67365 epoch 5; steps=10000/38898; loss=5.66463 epoch 5; steps=20000/38898; loss=5.82155 epoch 5; steps=30000/38898; loss=5.77744 epoch 5 ; lr=0.0025; loss=5.52081 epoch 6; steps= 0/38898; loss=5.54513 epoch 6; steps=10000/38898; loss=5.40478 epoch 6; steps=20000/38898; loss=5.71839 epoch 6; steps=30000/38898; loss=5.23344 epoch 6 ; lr=0.0025; loss=5.47719 epoch 7; steps= 0/38898; loss=4.93150 epoch 7; steps=10000/38898; loss=5.09374 epoch 7; steps=20000/38898; loss=5.47905 epoch 7; steps=30000/38898; loss=5.92455 epoch 7 ; lr=0.0025; loss=5.47128 epoch 8; steps= 0/38898; loss=5.22414 epoch 8; steps=10000/38898; loss=5.67435 epoch 8; steps=20000/38898; loss=5.55853 epoch 8; steps=30000/38898; loss=5.28791 epoch 8 ; lr=0.0025; loss=5.47099 epoch 9; steps= 0/38898; loss=5.33284 epoch 9; steps=10000/38898; loss=5.80998 epoch 9; steps=20000/38898; loss=6.04806 epoch 9; steps=30000/38898; loss=5.58979 epoch 9 ; lr=0.0025; loss=5.47106 epoch 10; steps= 0/38898; loss=5.09184 epoch 10; steps=10000/38898; loss=5.18391 epoch 10; steps=20000/38898; loss=5.66316 epoch 10; steps=30000/38898; loss=5.44824 epoch 10 ; lr=0.00125; loss=5.47111 1 2 weights = model.embedding.weight.detach().cpu().numpy() weights.shape (2103, 30) Creating the similarity matrix 1 similarity = cosine_similarity(weights, weights) 1 2 3 4 5 6 7 8 9 10 11 def get_similar_words(input_word: str, n: int): if input_word not in text_processor.w2ix: print(\u0026#34;word not in vocab\u0026#34;) else: input_word_ix = text_processor.w2ix[input_word] similarity_vector = similarity[input_word_ix] most_similar_ixs = np.argsort(similarity_vector)[-n:][::-1] most_similar_words = [(text_processor.ix2w[ix], similarity_vector[ix]) for ix in most_similar_ixs] for word, score in most_similar_words: print(f\u0026#34; \u0026gt; {word}, score={score: .2f}\u0026#34;) Exploring similar embeddings for some queries 1 get_similar_words(\u0026#34;snape\u0026#34;, 10) \u0026gt; snape, score= 1.00 \u0026gt; karkaroff, score= 0.81 \u0026gt; dumbledore, score= 0.79 \u0026gt; lupin, score= 0.77 \u0026gt; mcgonagall, score= 0.76 \u0026gt; moody, score= 0.74 \u0026gt; professor, score= 0.72 \u0026gt; flitwick, score= 0.71 \u0026gt; trelawney, score= 0.69 \u0026gt; quirrell, score= 0.69 1 get_similar_words(\u0026#34;vernon\u0026#34;, 10) \u0026gt; vernon, score= 1.00 \u0026gt; aunt, score= 0.81 \u0026gt; uncle, score= 0.79 \u0026gt; dudley, score= 0.78 \u0026gt; petunia, score= 0.70 \u0026gt; marge, score= 0.62 \u0026gt; furious, score= 0.59 \u0026gt; telephone, score= 0.53 \u0026gt; sister, score= 0.53 \u0026gt; company, score= 0.52 1 get_similar_words(\u0026#34;muggle\u0026#34;, 10) \u0026gt; muggle, score= 1.00 \u0026gt; wizarding, score= 0.81 \u0026gt; old, score= 0.65 \u0026gt; yer, score= 0.63 \u0026gt; international, score= 0.61 \u0026gt; important, score= 0.61 \u0026gt; our, score= 0.61 \u0026gt; is, score= 0.61 \u0026gt; young, score= 0.60 \u0026gt; yourself, score= 0.60 1 get_similar_words(\u0026#34;harry\u0026#34;, 10) \u0026gt; harry, score= 1.00 \u0026gt; lupin, score= 0.64 \u0026gt; ron, score= 0.63 \u0026gt; hermione, score= 0.61 \u0026gt; she, score= 0.57 \u0026gt; he, score= 0.57 \u0026gt; colin, score= 0.54 \u0026gt; dumbledore, score= 0.52 \u0026gt; furiously, score= 0.51 \u0026gt; moody, score= 0.49 1 get_similar_words(\u0026#34;slytherin\u0026#34;, 10) \u0026gt; slytherin, score= 1.00 \u0026gt; gryffindor, score= 0.79 \u0026gt; hufflepuff, score= 0.75 \u0026gt; team, score= 0.71 \u0026gt; heir, score= 0.71 \u0026gt; ravenclaw, score= 0.66 \u0026gt; goal, score= 0.64 \u0026gt; seeker, score= 0.64 \u0026gt; wood, score= 0.60 \u0026gt; irish, score= 0.60 1 get_similar_words(\u0026#34;malfoy\u0026#34;, 10) \u0026gt; malfoy, score= 1.00 \u0026gt; goyle, score= 0.77 \u0026gt; crabbe, score= 0.64 \u0026gt; draco, score= 0.61 \u0026gt; diggory, score= 0.59 \u0026gt; lucius, score= 0.57 \u0026gt; weasley, score= 0.51 \u0026gt; laughing, score= 0.50 \u0026gt; pettigrew, score= 0.50 \u0026gt; percy, score= 0.50 There are various Intrisic and Extrinsic evaluation criterias for these embeddings. For Evaluation and Interpretation of the embeddings take a look here.\n","date":"Mar 12","permalink":"https://iutsav.dev/posts/word2vec_skipgram/","tags":["python","word2vec","skipgram"],"title":"Revisiting Word2Vec skip-gram model"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://iutsav.dev/archives/","tags":null,"title":"Archive"}]
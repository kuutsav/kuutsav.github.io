<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Utsav&#39;Log</title>
    <link>https://iutsav.dev/tags/python/</link>
    <description>Recent content in python on Utsav&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Aug 2022 08:10:25 +0530</lastBuildDate><atom:link href="https://iutsav.dev/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Information Retrieval, Part 5 - Finetuning a Cross-Encoder</title>
      <link>https://iutsav.dev/posts/information_retrieval_5_finetuning_cross_encoder/</link>
      <pubDate>Sun, 07 Aug 2022 08:10:25 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_5_finetuning_cross_encoder/</guid>
      <description>In this notebook, we will finetune a Cross-Encoder on Semantic Textual Similarity text.
First let&amp;rsquo;s look at the architecture of a cross encoder.
In a Bi-Encoder, we pass both the sentences(A and B) separately to the finetuned model and use the pooled embeddings to compare the similarity between the sentences (cosine or dot product).
In a Cross-Encoder, we pass both the sentences together in the model and finetune the model with a linear head with output size 1, paired with a Binary Cross Entropy loss.</description>
    </item>
    
    <item>
      <title>Information Retrieval, Part 4 - Finetuning SBERT with MNR</title>
      <link>https://iutsav.dev/posts/information_retrieval_4_finetuning_sbert_with_mnr/</link>
      <pubDate>Fri, 05 Aug 2022 18:10:25 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_4_finetuning_sbert_with_mnr/</guid>
      <description>In this notebook we will finetune the bert-base model for semantic search using Multiple Negative Ranking loss.
This will be mostly similar to the finetuning we did in the previous notebook. The main changes are:
 We won&amp;rsquo;t use a fully connected layer on top of the embeddings now We will use Multiple Negative Ranking loss We will compute cosine similarity between the pooled u and v embedding and use that in MNR  MNR loss</description>
    </item>
    
    <item>
      <title>Information Retrieval, Part 3 - Finetuning BERT for IR</title>
      <link>https://iutsav.dev/posts/information_retrieval_3_finetuning_bert_for_ir/</link>
      <pubDate>Thu, 04 Aug 2022 13:15:20 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_3_finetuning_bert_for_ir/</guid>
      <description>We will briefly talk about sparse and dense represenations of texts and take a quick glance at techniques that came before the Transformers.
   Sparse representation of text: One-hot vectors  Traditionally we use a one-hot vectors to represent text as vectors. For example, for a sentence &amp;ldquo;a quick brown fox&amp;rdquo;, we could end up with following one hot vectors.
[1, 0, 0, 0] - a [0, 1, 0, 0] - quick [0, 0, 1, 1] - brown [0, 0, 0, 1] - fox  It&amp;rsquo;s easy to see that for larger vocabularies such as web pages, faq articles, etc.</description>
    </item>
    
    <item>
      <title>Information Retrieval, Part 2 - Evaluation metrics</title>
      <link>https://iutsav.dev/posts/information_retrieval_2_evaluation_metrics/</link>
      <pubDate>Thu, 04 Aug 2022 13:15:16 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_2_evaluation_metrics/</guid>
      <description>Here we are only going to look at offline evaluation metrics. We won&amp;rsquo;t cover online evaluation techniques like click-through rate or running A/B tests where a subset of users are presented results from a newer test system.
   Offline evaluation  The idea behind these evaluations is to quantitatively compare multiple IR models. Typically we have a labelled dataset where we have queries mapped to relvevant documents. The documents could either be graded or non-graded(binary).</description>
    </item>
    
    <item>
      <title>Information Retrieval, Part 1 - The Inverted Index</title>
      <link>https://iutsav.dev/posts/information_retrieval_1_classic_ir/</link>
      <pubDate>Mon, 01 Aug 2022 19:34:14 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_1_classic_ir/</guid>
      <description>IR in it&amp;rsquo;s most basic form answers the question &amp;ldquo;how relevant is a given query for a document&amp;rdquo;. The challenge is that we don&amp;rsquo;t have just 1 document but potentially millions or billions of documents. So the key challenge is - how can we efficiently find this &amp;ldquo;needle in the haystack&amp;rdquo; or the &amp;ldquo;relevant documents for a query&amp;rdquo;.
Here, document refers to any kind of text document, typically these could be web pages, emails, plain text documents, etc.</description>
    </item>
    
    <item>
      <title>Dynamic Connectivity and Percolation</title>
      <link>https://iutsav.dev/posts/dynamic_connectivity_and_percolation/</link>
      <pubDate>Thu, 07 Apr 2022 13:40:40 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/dynamic_connectivity_and_percolation/</guid>
      <description>Given a set of N objects that support the following two commands:
 Union: Connect two objects. Find/Connected: Is there a path connecting the two obejcts?  For example, consider this set of 10 objects
After few union commands union(2, 3), union(6, 5), union(8, 6), union(10, 8) the state of the system changes to
We can query the above system to find if two objects are connected or not like find(0, 1) == False, find(1, 2) == True, find(4, 9) == True, find(8, 1) == False</description>
    </item>
    
    <item>
      <title>Python Notes, Part 2 - Sequences</title>
      <link>https://iutsav.dev/posts/python_notes_2_sequences/</link>
      <pubDate>Tue, 05 Apr 2022 10:21:20 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/python_notes_2_sequences/</guid>
      <description>Python provides a variety of sequences; understanding these builtin sequences saves us from reinventing the wheel. We can also aspire to create APIs that support existing and user created sequence types. Python offers two types of sequences:
 Container sequences: These hold items of different types, for example list, tuple, etc. Flat sequences: These hold items of the same type, for example str, bytes, etc.  A container sequence holds references to objects where as a flat sequence contains the value of the contents in it&amp;rsquo;s own memory.</description>
    </item>
    
    <item>
      <title>Python Notes, Part 1 - Data Model</title>
      <link>https://iutsav.dev/posts/python_notes_1_datamodel/</link>
      <pubDate>Sat, 05 Mar 2022 10:09:25 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/python_notes_1_datamodel/</guid>
      <description>One of the best things about python is it&amp;rsquo;s consistency. Once someone has spent enough time with python and it&amp;rsquo;s ecosystem, they tend to adapt easily to new tools, modules as long as they are written in a pythonic way.
What is considered pythonic is a very subjective matter, but we can always look at the &amp;ldquo;Zen of Python&amp;rdquo; for some guidance.
1  import this   The Zen of Python, by Tim Peters Beautiful is better than ugly.</description>
    </item>
    
    <item>
      <title>Revisiting Word2Vec skip-gram model</title>
      <link>https://iutsav.dev/posts/word2vec_skipgram/</link>
      <pubDate>Fri, 12 Mar 2021 15:34:11 +1510</pubDate>
      
      <guid>https://iutsav.dev/posts/word2vec_skipgram/</guid>
      <description>In this post, we will train a Word2Vec skip-gram model from scratch on some text and inspect the trained embeddings at the end.
The first step for using any kind of NLP pipeline is to vectorize the text. Traditionally, we used to do this using one-hot representation of vectors. This had various downsides like:
 Vectors tend to be very long as their size depends on the vocabulary size of the corpus(which grows with the corpus size).</description>
    </item>
    
  </channel>
</rss>

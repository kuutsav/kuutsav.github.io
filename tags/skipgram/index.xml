<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>skipgram on Utsav&#39;Log</title>
    <link>https://iutsav.dev/tags/skipgram/</link>
    <description>Recent content in skipgram on Utsav&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Mar 2021 15:34:11 +1510</lastBuildDate><atom:link href="https://iutsav.dev/tags/skipgram/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Revisiting Word2Vec skip-gram model</title>
      <link>https://iutsav.dev/posts/word2vec_skipgram/</link>
      <pubDate>Fri, 12 Mar 2021 15:34:11 +1510</pubDate>
      
      <guid>https://iutsav.dev/posts/word2vec_skipgram/</guid>
      <description>In this post, we will train a Word2Vec skip-gram model from scratch on some text and inspect the trained embeddings at the end.
The first step for using any kind of NLP pipeline is to vectorize the text. Traditionally, we used to do this using one-hot representation of vectors. This had various downsides like:
 Vectors tend to be very long as their size depends on the vocabulary size of the corpus(which grows with the corpus size).</description>
    </item>
    
  </channel>
</rss>

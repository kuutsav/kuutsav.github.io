<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>evaluation on Utsav&#39;Log</title>
    <link>https://iutsav.dev/tags/evaluation/</link>
    <description>Recent content in evaluation on Utsav&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Aug 2022 13:15:16 +0530</lastBuildDate><atom:link href="https://iutsav.dev/tags/evaluation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Information Retrieval, Part 2 - Evaluation metrics</title>
      <link>https://iutsav.dev/posts/information_retrieval_2_evaluation_metrics/</link>
      <pubDate>Thu, 04 Aug 2022 13:15:16 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_2_evaluation_metrics/</guid>
      <description>Here we are only going to look at offline evaluation metrics. We won&amp;rsquo;t cover online evaluation techniques like click-through rate or running A/B tests where a subset of users are presented results from a newer test system.
   Offline evaluation  The idea behind these evaluations is to quantitatively compare multiple IR models. Typically we have a labelled dataset where we have queries mapped to relvevant documents. The documents could either be graded or non-graded(binary).</description>
    </item>
    
  </channel>
</rss>

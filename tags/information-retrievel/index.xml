<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>information-retrievel on Utsav&#39;Log</title>
    <link>https://iutsav.dev/tags/information-retrievel/</link>
    <description>Recent content in information-retrievel on Utsav&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Aug 2022 13:15:20 +0530</lastBuildDate><atom:link href="https://iutsav.dev/tags/information-retrievel/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Information Retrieval, Part 3 - Finetuning BERT for IR</title>
      <link>https://iutsav.dev/posts/information_retrieval_3_finetuning_bert_for_ir/</link>
      <pubDate>Thu, 04 Aug 2022 13:15:20 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_3_finetuning_bert_for_ir/</guid>
      <description>We will briefly talk about sparse and dense represenations of texts and take a quick glance at techniques that came before the Transformers.
   Sparse representation of text: One-hot vectors  Traditionally we use a one-hot vectors to represent text as vectors. For example, for a sentence &amp;ldquo;a quick brown fox&amp;rdquo;, we could end up with following one hot vectors.
[1, 0, 0, 0] - a [0, 1, 0, 0] - quick [0, 0, 1, 1] - brown [0, 0, 0, 1] - fox  It&amp;rsquo;s easy to see that for larger vocabularies such as web pages, faq articles, etc.</description>
    </item>
    
    <item>
      <title>Information Retrieval, Part 2 - Evaluation metrics</title>
      <link>https://iutsav.dev/posts/information_retrieval_2_evaluation_metrics/</link>
      <pubDate>Thu, 04 Aug 2022 13:15:16 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_2_evaluation_metrics/</guid>
      <description>Here we are only going to look at offline evaluation metrics. We won&amp;rsquo;t cover online evaluation techniques like click-through rate or running A/B tests where a subset of users are presented results from a newer test system.
   Offline evaluation  The idea behind these evaluations is to quantitatively compare multiple IR models. Typically we have a labelled dataset where we have queries mapped to relvevant documents. The documents could either be graded or non-graded(binary).</description>
    </item>
    
    <item>
      <title>Information Retrieval, Part 1 - The Inverted Index</title>
      <link>https://iutsav.dev/posts/information_retrieval_1_classic_ir/</link>
      <pubDate>Mon, 01 Aug 2022 19:34:14 +0530</pubDate>
      
      <guid>https://iutsav.dev/posts/information_retrieval_1_classic_ir/</guid>
      <description>IR in it&amp;rsquo;s most basic form answers the question &amp;ldquo;how relevant is a given query for a document&amp;rdquo;. The challenge is that we don&amp;rsquo;t have just 1 document but potentially millions or billions of documents. So the key challenge is - how can we efficiently find this &amp;ldquo;needle in the haystack&amp;rdquo; or the &amp;ldquo;relevant documents for a query&amp;rdquo;.
Here, document refers to any kind of text document, typically these could be web pages, emails, plain text documents, etc.</description>
    </item>
    
  </channel>
</rss>
